---
title: "lat_models_play"
author: "DVM Bishop"
date: "31/05/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(Hmisc)
```

# Background

##Laterality at the level of the population and the individual
In considering whether or not language laterality is unitary, we need to distinguish between predictions about task-dependent aspects of language laterality in the population as a whole, as opposed to individual differences in lateralisation. Most theories of language lateralisation have focused on how language functions are lateralised in the brain in typical humans, without regard to individual differences. For instance, Hickok and Poeppel's (2007) dual route model of speech processing contrasts a dorsal stream from superior temporal to premotor cortices via the arcuate fasciculus, which is associated with sensorimotor integration of auditory speech sounds and articulatory motor actions; and a ventral stream from temporal cortex to anterior inferior frontal gyrus, which is involved in access to conceptual memory and identification of auditory objects (Rauschecker, 2018). Hickok and Poeppel proposed that the dorsal stream is left lateralized, whereas the ventral stream is bilateral.  This kind of theory makes predictions about task-related differences that can be assessed by comparing mean laterality indices in a sample. Thus, the prediction from the dual route model is that mean laterality indices for tasks involving the dorsal stream will show left-lateralisation, whereas indices from tasks primarily involving the ventral stream will not.  In practice, our ability to test this prediction depends on having reliable tasks that load predominantly on one stream or the other.

The picture is complicated, however, by individual differences; as well as the typical pattern of language laterality, some individuals show the reverse pattern - right-hemisphere language. A subset of people with bilateral language has also been described, but this is not well-specified. These could be people who are strongly lateralized, but with different direction of lateralization for different tasks, or people who engage both hemispheres equally during language tasks. It may in practice be difficult to distinguish the latter situation from poor test reliability, where noisy measurement masks genuine lateralization.  To gain insights into individual differences, we need to move away from a focus on mean laterality indices, to look at covariances between indices on different measures. Laterality indices across tasks may be positively correlated even if overall left-lateralisation is weak or absent, if the population consists of a mixture of those who are reliably left-lateralised and those who are right-lateralised. At the level of population means, this would look like bilateral function, but the mean would misrepresent the true situation.

Viewed this way, it looks as if we could model our data with a classic regression approach, where an observed score is determined by 3 terms:

a - an intercept that corresponds to a task-specific population bias

b - a term corresponding to stable individual differences

e - an error term

And I guess one question then is whether there is evidence of a x b, which would be significant if there were people whose personal laterality varied from task to task

Need to look at correlations to see what it looks like; vary the weighting of task and individual terms

This simulation allows for task-specific bias between tasks, and an individual-specific bias that is constant.

Without either bias, data is random: no reliability and no laterality

With only task bias, one sees task effects but no reliability

With only subject bias, no task effect but reliable

However, with this simulation the reliabilities are similar to cross-task correlations.
This is because the subject term is the same for all! The different means don't affect the correlation.

This is the original simulation which uses 6 tasks and just compares models where task term  can be 0 or 1, and subject term can be 0 or 1.
We can manipulate the weights of these to get desired means.

NB at this point, not including interactions.



```{r modelsim, echo=FALSE}


nsub <- 5000 #use large N to get accurate prediction of r values
errwt <- 1 #weighting for error term
mycondition <-0
fordemo <-data.frame(matrix(NA,nrow=nsub,ncol=24))#holds data for demo plots
colnames(fordemo)<-c('A1.1','A2.1','E1.1','E2.1','F1.1','F2.1',
                     'A1.2','A2.2','E1.2','E2.2','F1.2','F2.2',
                     'A1.3','A2.3','E1.3','E2.3','F1.3','F2.3',
                     'A1.4','A2.4','E1.4','E2.4','F1.4','F2.4')
for (taskwt in c(0,1)){
  for (indwt in c(0,1)){
    mycondition<-mycondition+1 #keep track of different conditions
print(paste('Task weighting is ',taskwt))
print (paste('Subject weighting is ', indwt))

taskeffect <-c(2,1.6,1.4,1.2,1,0) #simulate 6 tasks
indeffect <- rnorm(nsub) #simulate nsub people
thisrow <- 0
myscore <- data.frame(matrix(NA,nrow=length(indeffect),ncol=2*length(taskeffect)+1))
colnames(myscore) <- c('ID','A1','A2','B1','B2','C1','C2','D1','D2','E1','E2','F1','F2')
  for (j in 1: length(indeffect)){
    thisrow<-thisrow+1
    thiscol <- 1
    myscore[thisrow,thiscol]<-thisrow
    for (i in 1:length(taskeffect)){
    for (k in 1:2){ #2 test occasions
      thiscol<-thiscol+1
    # The next line simulates situation with task x person interaction
    #  myscore[thisrow,thiscol]<-taskeffect[i]+indeffect[j]+taskeffect[i]*indeffect[j]+rnorm(1)
     #  The next line simulates situation without task x person interaction
       myscore[thisrow,thiscol]<-taskwt*taskeffect[i]+indwt*indeffect[j]+errwt*rnorm(1)
   
    }
      
  }
  }
if(taskwt==0){myscore<-myscore+1} #just to make more realistic - ie overall bias
mycolrange<-(6*(mycondition-1)+1):(6*mycondition)
fordemo[,mycolrange]<-cbind(myscore$A1,myscore$A2,myscore$E1,myscore$E2,myscore$F1,myscore$F2)
#in terms of correlations and means, data look v like real data!
mymeans <- round(colMeans(myscore[,2:ncol(myscore)]),3)
print('Task means')
print(mymeans)

mycorr <-rcorr(as.matrix(myscore[,2:ncol(myscore)]))
myreliab <- c(mycorr$r[1,2],mycorr$r[3,4],mycorr$r[5,6],mycorr$r[7,8],mycorr$r[9,10],mycorr$r[11,12])
print('reliabilities')
print(myreliab)
print('Median reliability')
print(median(myreliab))
mycrosstask <- c (mycorr$r[1,3],mycorr$r[1,5],mycorr$r[1,7],mycorr$r[1,9],mycorr$r[1,11],
                       mycorr$r[3,5],mycorr$r[3,7],mycorr$r[3,9],mycorr$r[3,11],
                      mycorr$r[5,7],mycorr$r[5,9],mycorr$r[5,11],
                      mycorr$r[7,9],mycorr$r[7,11],
                      mycorr$r[9,11])
print('Cross-task correlations')
print(mycrosstask)
print('Median cross-task correlation')
print(median(mycrosstask))
  }
}
```

The simulations confirm that the means don't affect the correlations - ie task model and subject model are independent.

```{r demoplot}
#Initial attempt to look at plots of within-task vs between-task correlations
#We just consider tasks A and E - these have different LI means.
for (k in 1:4){
  pdf(paste0("SampleGraph",k,".pdf"),width=7,height=7)
  kbase<-6*(k-1) #NB there are 6 cols but we only plot 4
par(mfrow=c(2,2))
mypairs<-matrix(c(1,2,1,3,3,4,2,4),nrow=4,byrow=TRUE)
  for (i in 1:4){
      a<-mypairs[i,1]+kbase
      b<-mypairs[i,2]+kbase
#NB we only plot first 100 cases, but correlations computed for the whole dataset
    plot(fordemo[1:100,a],fordemo[1:100,b],xlab=colnames(fordemo)[a],ylab=colnames(fordemo)[b],xlim=c(-3,5),ylim=c(-3,5),cex=.75)
   abline(h=mean(fordemo[,a]))
   abline(v=mean(fordemo[,b]))
   rbit<-paste('r = ',round(cor(fordemo[,a],fordemo[b]),3))
   text(-2,3.5,rbit)
  }
dev.off()
}
```

This has clarified that the key points to emphasise in a figure are that:
a) Means and covariances are independent - adding subject term to model affects covariance, and adding task term affects means. 
b) Differing means don't affect covariances
c) Correlations within-task and between-task don't differ unless interaction term (not previously modeled)

So we need a nice figure showing a sample within-task and between-task correlation, with differing means, for models with increasing N terms


```{r demoplotshort}
 pdf(paste0("SimModels.pdf"),width=5,height=7)
par(mfrow=c(4,2)) 
#par(mar=c(3.5,4,1,1)) #bottom , left, top , right
#see: https://stackoverflow.com/questions/19422625/mtext-y-axis-label-that-covers-a-two-tile-plot
for (k in 1:5){ #cycle through each model: one per row
  par(mar = c(3,3,0,0)) # regular margin: use outer margin below for overarching legends etc

  par(oma=c(3.5, 4, 2, 1)) #outer margin specified for each row - but this then starts new page for each row
  #But if oma is specified before loop, then all titles written on top of each other
 if(k != 2){ #we don't need model 2 - this is subj effect and no task effect
 
    kbase<-6*(k-1) #NB there are 6 cols (3 vars) but we only plot 2 vars, A and E


#for the interaction condition, create a column for E variables in different order
    #This means both variables still reliable, but they don't intercorrelate
fordemo$A1.5<-fordemo$A1.4
fordemo$A2.5<-fordemo$A2.4
myrank<-rank(fordemo$E1.4)
fordemo$E1.5<-fordemo$E1.4[myrank]
fordemo$E2.5<-fordemo$E2.4[myrank]
mypairs<-matrix(c(1,2,1,3),nrow=2,byrow=TRUE)
  for (i in 1:2){
      a<-mypairs[i,1]+kbase
      b<-mypairs[i,2]+kbase
myxlab<-'LI Task A(1)'
myylab<-'LI Task A(2)'
if(i==2){myylab<-'Task B(1)'}
    plot(fordemo[1:150,a],fordemo[1:150,b],xlim=c(-3,5),ylim=c(-3,5),pch=16,col='blue',frame.plot=FALSE,cex.axis=.7,xlab=myxlab,ylab=myylab)
    #for subscript labels see https://stackoverflow.com/questions/10156417/subscripts-in-plots-in-r
    #These commands also ensure label close to axis with line =2 command
    if (k==1){
    title(expression('LI'['ij']*' = a + e'['ij']),outer=TRUE)
     }
        if (k==3){
    title(expression('LI'['ij']*' = a + t'['i']*' + e'['ij']),outer=TRUE)}
            if (k==4){
    title(expression('LI'['ij']*' = a + t'['i']*' + p'['j']*' + e'['ij']),outer=TRUE)}
               if (k==5){
    title(expression('LI'['ij']*' = a + t'['i']*' + p'['j']*' + x'['ij']*' + e'['ij']),outer=TRUE)}

    title(ylab=myylab,line=2,outer=FALSE)
       title(xlab=myxlab,line=2)
    #previously used abline, but lines extend too far: segments allows control of start/end
  segments(y0=-3,y1=4,x0=mean(fordemo[,b]),x1=mean(fordemo[,b]),lty=2,col='red')
   segments(x0=-3,x1=4,y0=mean(fordemo[,b]),y1=mean(fordemo[,b]),lty=2,col='red')
   segments(y0=-3,y1=4,x0=0,x1=0,col='grey')
   segments(x0=-3,x1=4,y0=0,y1=0,col='grey')

   rbit<-paste('r = ',round(cor(fordemo[,a],fordemo[b]),1))
   prebit<-'Test-retest \n'
   if(i==2){prebit <- 'Cross-task \n'}
   text(-3,3.5,paste(prebit,rbit),pos=4) #pos=4 specifies left-justify
  }

 }
}
dev.off()
```
What about real data?!
Are the correlations higher for same task vs cross-task?

Reliability = within-task correlation is:
`r median(c(.05,.57,.71,.7,.53,.72))`

Between-task correlation (session 1) is:
`r median(c(.24,.38,.52,.47,.18,.51,.57,.65,.49,.53,.54,.3,.51,.33,.62))`


Between-task correlation (session 2) is:
`r median(c(.23,.2,.25,.08,.21,.71,.54,.65,.39,.76,.65,.37,.5,.33,.45))`

Cross-task, cross-session 1 to 2 is:
`r median(c(.42,.31,.24,.47,.35,.65,.56,.27,.44,.62,.56,.31,.55,.43,.62))`

Cross-task, cross-session 2 to 1 is:
`r median(c(.24,.21,.28,.12,.2,.63,.55,.68,.41,.55,.60,.31,.5,.2,.2))`

It looks as if in general the correlations are higher for within-task, but rather marginal, given the error of measurement. Some of the cross-task correlations are above .6

If within-task correlation is higher than between-task, that suggests there are different subject effects for different tasks. Task A is problematic because low within-task anyhow - at Time 1 the correlation is actually higher with other tasks than with same task!

Question is what do factor scores look like for the two extracted factors?
Can we plot these?
Need to look at A2_5_SEM.R for this.

Have appended factor scores and plots to A2_5_SEM.R: redone here

```{r facscores}
require(Hmisc)
# Use significant paths from SEM to compute factors
lidat<-read.csv('LI_data2.csv')
alltask<-cbind(lidat[1:30,3:8],lidat[31:60,3:8])
colnames(alltask)<-c('ListGen1','PhonDec1','SemDec1','SentGen1','SentComp1','Jabber1',
            'ListGen2','PhonDec2','SemDec2','SentGen2','SentComp2','Jabber2')


alltask$FacA.1 <- alltask$ListGen1+
                  2.07*alltask$PhonDec1+
                  1.95*alltask$SemDec1+
                  2.86*alltask$SentGen1+
                  2.07*alltask$SentComp1
alltask$FacA.2 <- alltask$ListGen2+
  2.07*alltask$PhonDec2+
  1.95*alltask$SemDec2+
  2.86*alltask$SentGen2+
  2.07*alltask$SentComp2

alltask$FacB.1 <-.73*alltask$SentComp1+.82*alltask$Jabber1
alltask$FacB.2 <-.73*alltask$SentComp2+.82*alltask$Jabber2
par(mfrow=c(2,2))
plot(alltask$FacA.1,alltask$FacA.2)
plot(alltask$FacB.1,alltask$FacB.2)
plot(alltask$FacA.1,alltask$FacB.1)
plot(alltask$FacA.2,alltask$FacB.2)

myncol<-ncol(alltask)
rcorr(as.matrix(alltask[,(myncol-3):myncol]))

alltask$meansess1 <- rowMeans(alltask[,1:6])
alltask$meansess2 <- rowMeans(alltask[,7:12])
cor(alltask$meansess1,alltask$meansess2)
plot(alltask$meansess1,alltask$meansess2)
```


Shows good reliability for first factor (r=.86), less so for 2nd factor (r = .64).
Also factors are positively correlated - around .71.
A factor based on mean for all tests is also v reliable - mean similar to just factor 1.
Does question whether bifactor is really much better!

This approach also suggests linear mixed models would make sense: will try this for converging evidence.

```{R lmmapproach}
require(lme4)
library(car)
lidat<-read.csv('LI_data2.csv')
myrow<-nrow(lidat)

#reshape into long form
longdat <- rbind(lidat[,1:2],lidat[,1:2],lidat[,1:2],lidat[,1:2],lidat[,1:2],lidat[,1:2])
longdat$task <-c(rep('A',myrow),rep('B',myrow),rep('C',myrow),rep('D',myrow),rep('E',myrow),rep('F',myrow))
longdat$li <-c(lidat$A,lidat$B,lidat$C,lidat$D,lidat$E,lidat$F)
longdat$task<-as.factor(longdat$task)
lmfull <- lmer(li ~ Session+ task + (1 +task|ID) , data = longdat,
    REML = FALSE) #model where as well as task effect, there is random effect of ID which varies by task
summary(lmfull)
lmm <- lmer(li ~ Session+ task + (1 |ID) , data = longdat,
    REML = FALSE)#model where as well as task effect, there is random effect of ID 
summary(lmm)
lm.a <- lm(li ~ Session+ task , data = longdat) #model with no random effect of ID
summary(lm.a)
print('\nCompare the full model with model without task-varying random effect')
anova(lmfull,lmm)
print('\nCompare the model with constant random effect vs no random effect')
anova(lmm,lm.a)

```
This shows that model with random effect of ID varying by task is better fit than one with constant random effect.
Model with no random effect is poor fit.

Our factor model suggests optimal may be to treat tasks A-D as one block and E-F as another.
Rerun model, this time using 'taskblock' rather than task.

```{r blockedtask}
longdat$taskblock<-1
w <- c(which(longdat$task=='E'),which(longdat$task=='F'))
longdat$taskblock[w]<-2
longdat$taskblock<-as.factor(longdat$taskblock)
lmbyblock <- lmer(li ~ Session+ task + (1 +taskblock|ID) , data = longdat,
    REML = FALSE) #model where as well as task effect, there is random effect of ID which varies by taskblock
summary(lmbyblock)
lmfull <- lmer(li ~ Session+ task + (1 +task|ID) , data = longdat,
    REML = FALSE)#model where as well as task effect, there is random effect of ID which varies by task
summary(lmfull)
print('\nCompare the model with random effect by block and model with random effect by task')
anova(lmfull,lmbyblock)


```

BIC is lower for model with blocked tasks than separate tasks.
What about if task F is only one in block 2?

```{r blockFsep}
longdat$taskblock<-1
w <- which(longdat$task=='F')
longdat$taskblock[w]<-2
longdat$taskblock<-as.factor(longdat$taskblock)
lmbyblock <- lmer(li ~ Session+ task + (1 +taskblock|ID) , data = longdat,
    REML = FALSE) #model where as well as task effect, there is random effect of ID which varies by taskblock
summary(lmbyblock)
lmfull <- lmer(li ~ Session+ task + (1 +task|ID) , data = longdat,
    REML = FALSE)#model where as well as task effect, there is random effect of ID which varies by task
summary(lmfull)
print('\nCompare the model with random effect by block and model with random effect by task')
anova(lmfull,lmbyblock)


```

Picture is similar.