---
title: "Analysis for A2 writeup"
author: "Dorothy Bishop/ Zoe Woodhead / Paul Thompson"
date: "28 Jul 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

`r Sys.time()`

## Background
Original protocol for project is here: <https://osf.io/tkpm2/registrations/>

See Dopp_explore_log.RMd for more background on how Modeling was developed.

Source reference is: Kline, R. B. (2011). Principles and practice of structural equation Modeling, 3rd edition. New York: Guilford Press.
Kline has detailed discussion of testing for fit: is against absolute cutoffs, though notes cutoffs that have been proposed on basis of simulation studies.

RMSEA is absolute index of (badness of) Model fit: aim for below .08.
This value decreases with more DF (greater parsimony) or larger sample size.
 
Comparative fit index (CFI) measures relative improvement in fit of Model over baseline Model. 
Computed as 1 - (chisqM - dfM)/(chisqB-dfB); Good fit if CFI > .95 (see Kline p 208)

##Chunks of code for setting packages and reading in data 
from SEM2_maxLikelihood_realdata.R
```{r packages, warning=FALSE, message=FALSE}
#Needs OpenMx, which you get with following command (not CRAN)
#Hmm - having problems with OpenMx - now using Cran version

#source('https://openmx.ssri.psu.edu/software/getOpenMx.R')
require(tidyverse)
require(OpenMx)
require(stargazer) #simple commands for nice tables
require(semTools) #for fit measures
library(DiagrammeR) #for the diagram
require(stringr)
require(data.table)
library('devtools')
library("reshape2")
library("yarrr")
library(DiagrammeRsvg)
library(magrittr)
library(svglite)
library(rsvg)

########### Set toggles before running! ################################################################
RHonly            <- 1 # Set to 0 if want to include L handers
run_participants  <- 0 # Set to 1 to caluclate age and gender stats of participant group
run_behavioural   <- 0 # Set to 1 to analyse behavioural data
usemean           <- 1 # Set to 0 to analyse original peak-based LI values
remove.Ntrial     <- 1 # Set to 1 to run exclusion of those with < 12 good trials
tryreorder        <- 1 # Set to 1 to change the fixed path in the SEM model
run_normalcheck   <- 0 # Set to 1 to run normality tests on LI data
run_pirate        <- 0 # Set to 1 to run pirate plot of LI data
run_corrheat      <- 0 # Set to 1 to run correlation matrix heatmap plot
run_pathdrop      <- 1 # Set to 1 to run pathdrop SEM analysis
run_covexplore    <- 1 # Set to 1 to run covariance exploration analysis
run_drawpaths     <- 1 # Set to 1 to run graphviz paths plot
run_dropone       <- 1 # Set to 1 to run drop-one SEM analysis
run_facscores     <- 1 # Set to 1 to run factor scores plot after drop one analysis (N.B. Only runs is run_dropone is also 1)

nuorder           <-   # Sets the new order of tasks. The task listed first (and seventh) will be fixed.
# c(3,1,2,4,5,6,9,7,8,10,11,12) # Semantic decision fixed
# c(2,1,3,4,5,6,8,7,9,10,11,12) # Phonological decision fixed
 c(4,1,2,3,5,6,10,7,8,9,11,12) # Sentence generation fixed
# c(5,1,2,3,4,6,11,7,8,9,10,12) # Sentence comprehension fixed
# c(6,1,2,3,4,5,12,7,8,9,10,11) # Jabberwocky decision fixed

########################################################################################################

```
##Code for setting directory flexibly
```{r definedir}
#This not currently used but should be easy to adapt if needed. 
#recommend just setting working directory to current folder where files reside
thisuser <-'dorothybishop'
thisdrive<- '' #may need to set this to 'C:' for PC users
#readdir <-paste0(thisdrive,'/Users/',thisuser,'/Dropbox/project A2/')
readdir<-'' #use this if working directory set to folder with all the data
```

##Participant demographic data
```{r participants}

particdat<-read.csv('A2_Participant_Info.csv')
rhanded<-which(particdat$handedness=='R')

if (RHonly==1){ #See RHonly toggle
  print('R handers only included in analysis')
  particdat <- particdat[rhanded,]
}
  
if(run_participants==1){ #See run_participants toggle
  #Participant descriptors reported in paper
  print('Gender x handedness')
  table(particdat$Gender,particdat$handedness)
  ageyr<-particdat$Age_m/12
  print('Age in yr')
  summary(ageyr)
}

```

##Task performance data
```{r behaviouraldata}

if (run_behavioural==1){ #See run_behavioural toggle
  
  behavdat<-read.csv('A2_Behavioural_Data.csv')
  
  if (RHonly==1){ #See RHonly toggle
    behavdat<-behavdat[rhanded,] #remove some blank lines at end of file
  }
  
  #word data
  word.cols<-grep('*.Words',colnames(behavdat))
  stargazer(behavdat[,word.cols],type='text')
  
  #accuracy data
  acc.cols<-grep('*.Acc', colnames(behavdat))
  print('Percentages correct')
  stargazer(behavdat[,acc.cols],type='text')
  
  #RT data
  rt.cols<-grep('*.RT', colnames(behavdat))
  print('RT')
  stargazer(behavdat[,rt.cols],type='text')
  
  #omit data
  omit.cols<-grep('*.Omit',colnames(behavdat))
  print('Omit')
  stargazer(behavdat[,omit.cols],type='text')
  
  #make data frame to hold data formatted for paper
  behavdf<-data.frame(matrix(NA,nrow=4,ncol=8))
  colnames(behavdf)<-c('%corr.1','%corr.2','t','p','RT.1','RT.2','t','p')
  
  #get means and SDs for table
  mymeanp<-round(sapply(behavdat[,acc.cols],mean,na.rm=TRUE),1)
  mysdp<-round(sapply(behavdat[,acc.cols],sd,na.rm=TRUE),2)
  mymeanr<-round(sapply(behavdat[,rt.cols],mean,na.rm=TRUE),2)
  mysdr<-round(sapply(behavdat[,rt.cols],sd,na.rm=TRUE),2)
  
  for (i in 1:4){
    
    behavdf[i,1]<-paste0(mymeanp[i],' (',mysdp[i],')')
    behavdf[i,2]<-paste0(mymeanp[(i+4)],' (',mysdp[i+4],')')
    behavdf[i,5]<-paste0(mymeanr[i],' (',mysdr[i],')')
    behavdf[i,6]<-paste0(mymeanr[(i+4)],' (',mysdr[i+4],')')
    thist<-t.test(behavdat[,acc.cols[i]],behavdat[,acc.cols[i+4]],paired=TRUE)
    behavdf[i,3]<-round(thist$statistic,2)
    behavdf[i,4]<-round(thist$p.value,3)
    thist2<-t.test(behavdat[,rt.cols[i]],behavdat[,rt.cols[i+4]],paired=TRUE)
    behavdf[i,7]<-round(thist2$statistic,2)
    behavdf[i,8]<-round(thist2$p.value,3)
  }
  rownames(behavdf)<-c('B','C','E','F')
  write.table(behavdf, "Behavtable.txt", sep="\t",row.names=TRUE,quote=FALSE) #Currently this is table 1
}

```

##Code for reading and reshaping LI data
N.B. Can select to use original LI values based on peak, or LI values from mean in POI.

```{r readdata}
#read in data from sessions 1 and 2, having saved as .csv
#NB. Need to set working directory to location of data files - or else specify path

data1<-read.csv('Results_Session1.csv')
data2<-read.csv('Results_Session2.csv')

alltaskall <- cbind(select(data1,A1.LI,B1.LI,C1.LI,D1.LI,E1.LI,F1.LI),
             select(data2,A2.LI,B2.LI,C2.LI,D2.LI,E2.LI,F2.LI))

if(usemean==1){ #See usemean toggle
  alltaskpeak <- alltaskall # Keep peak values for comparison
  alltaskall <- cbind(select(data1,A1.LI_mean,B1.LI_mean,C1.LI_mean,D1.LI_mean,E1.LI_mean,F1.LI_mean),
                      select(data2,A2.LI_mean,B2.LI_mean,C2.LI_mean,D2.LI_mean,E2.LI_mean,F2.LI_mean))
}

mylabels<-c('ListGen1','PhonDec1','SemDec1','SentGen1','SentComp1','Jabber1',
            'ListGen2','PhonDec2','SemDec2','SentGen2','SentComp2','Jabber2')
myshortlab<-c('ListGen','PhonDec','SemDec','SentGen','SentComp','Jabber')

myfixed<-myshortlab[nuorder[1]] ### Keep an eye on this......

colnames(alltaskall)<-mylabels
head(alltaskall)

if (RHonly==1){
  alltaskall<-alltaskall[rhanded,]
  if (usemean == 1){
    alltaskpeak <- alltaskpeak[rhanded,]
  }
}

```


##Code for identifying exclusions and substituting NA based on SE
Please see justification in Methods. This differs from what we preregistered but we can defend it as making more sense. I.e. we want to exclude runs where the data are unreliable, but we do not want to exclude runs where the LI is statistically unusual.

```{r excludeSE}

alltask<-alltaskall
#create a new copy where the outliers will be coded as NA

allse <- cbind(select(data1,A1.se,B1.se,C1.se,D1.se,E1.se,F1.se),
             select(data2,A2.se,B2.se,C2.se,D2.se,E2.se,F2.se))

if (usemean==1){ #See usemean toggle
  allse <- cbind(select(data1,A1.mean_se,B1.mean_se,C1.mean_se,D1.mean_se,E1.mean_se,F1.mean_se),
                 select(data2,A2.mean_se,B2.mean_se,C2.mean_se,D2.mean_se,E2.mean_se,F2.mean_se))
}

if (RHonly==1){ #See RHonly toggle
  allse<-allse[rhanded,]
}

myse<-c(allse[,1],allse[,2],allse[,3],allse[,4],allse[,5],allse[,6],
        allse[,7],allse[,8],allse[,9],allse[,10],allse[,11],allse[,12])

Q3<-quantile(myse,.75)
Q1<-quantile(myse,.25)
Qlimit<-Q3+2.2*(Q3-Q1)
secols<-colnames(allse)
dropSE<-0 #initialise counter

for (i in 1:12){
  w<-which(allse[,i]>Qlimit)
  if (length(w)>0){
    alltask[w,i]<-NA
    dropSE <- dropSE + sum(is.na(alltask[,i]))
  }
}

print(paste('Dropped because high SE, N = ',dropSE))

#Now remove those with fewer than 12 trials in a condition
if (remove.Ntrial==1){ #See remove.Ntrial toggle
  
  nbit<-c('A1.N','B1.N','C1.N','D1.N','E1.N','F1.N')
  includesubs<-1:nrow(data1)
  if(RHonly==1){
    includesubs<-rhanded
  }
  
  dropN<-0 #initialise counter
  for (i in 1:6){
    w<-which(colnames(data1)==nbit[i])
    ww<-which(data1[includesubs,w]<12)
    if(length(ww>0)){
      alltask[ww,i]<-NA
      dropN <- dropN + sum(data1[includesubs,w] < 12)
    }
  }
  
  nbit<-c('A2.N','B2.N','C2.N','D2.N','E2.N','F2.N')
  for (i in 1:6){
    w<-which(colnames(data2)==nbit[i])
    ww<-which(data2[includesubs,w]<12)
    if(length(ww>0)){
      alltask[ww,(i+6)]<-NA
      dropN <- dropN + sum(data2[includesubs,w] < 12)
    }
  }
  print(paste('Dropped because < 12 trials, N = ',dropN))  
}

```
##Check correlation with peak LI values
```{r peakcorr}
if (usemean == 1){ # Only runs if we're using mean values for LI
  allmeanLI <- numeric(0)
  allpeakLI <- numeric(0)
  for (i in 1:dim(alltask)[2]){
    allmeanLI <- c(allmeanLI, alltask[ ,i])
    allpeakLI <- c(allpeakLI, alltaskpeak[,i])
  }
  cor(allmeanLI, allpeakLI, use='pairwise.complete.obs', method='spearman') #Option: can switch method to 'spearman' or 'pearson'
  
}
```


##Try with different order of variables

```{r reordertasks}
#Reassuring that reordering variables does not affect model fit
#However, it does affect the model structure.
#The first variable in the list will have fixed paths to factor A (1) and B (0)

if (tryreorder==1){ #See tryreorder toggle
  alltask<-alltask[,nuorder] #See nuorder toggle
  alltaskall<-alltaskall[,nuorder]
  mylabels<-mylabels[nuorder]
  myshortlab<-myshortlab[nuorder[1:length(myshortlab)]]
}

```

##Show means etc for tasks with time1 and time2 adjacent
```{r stargaze}

stargazer(alltask[,c(1,7,2,8,3,9,4,10,5,11,6,12)],type='text')

```

##Check normality of data
Have a look at the densities and do Shapiro-Wilks test and QQ plot
Plots will be written to the working directory, called densities 1-6
```{r normalcheck}

if (run_normalcheck==1){ # See run_normalcheck toggle
  for (i in 1:6){ # Loop through tasks
    # png(filename=paste0("densities_",i,".png"))
    par(mfrow=c(2,2))
    
    for (j in 1:2){ # Loop through sessions
      offset<- 6*(j-1)
      tempdata<-alltask[,(i+offset)]
      myshap<-shapiro.test(tempdata)
      plot(density(tempdata,na.rm=TRUE),main=mylabels[(i+offset)],xlim=c(-6,8),ylim=c(0,.3))
      text(-4,.1,paste('mean = \n',round(mean(tempdata,na.rm=TRUE),2)))
      
      ## Plot using a qqplot
      qqnorm(tempdata);qqline(tempdata, col = 2)
      text(.4,0,paste('Shapiro\np = ',round(myshap$p.value,3)))
    }
    # dev.off()
  }
}
```
##Pirateplot

```{r dopirate}
myrow<-nrow(alltask)

if (run_pirate == 1){ # See run_pirate toggle
  
  #First do one sample t-tests to check if sig different from zero
  tdf<-data.frame(matrix(NA,nrow=12,ncol=5))
  colnames(tdf)<-c('t','p','labelht','label','task')
  ilist<-c(1,7,2,8,3,9,4,10,5,11,6,12)
  
  for (j in 1:12){
    i<-ilist[j]
    myt<-t.test(alltask[,i])
    tdf[j,1]<-round(myt$statistic,2)
    tdf[j,2]<-round(myt$p.value,3)
    tdf[j,3]<-max(alltask[,i],na.rm=TRUE)+1
    tdf[j,4]<-''
    if(myt$p.value<.05){tdf[j,4]<-"*"}
    if(myt$p.value<.01){tdf[j,4]<-"**"}
    if(myt$p.value<.001){tdf[j,4]<-"***"}
    tdf[j,5]<-colnames(alltask)[i]
  }
  
  # Melt cleverly reshapes LI_data into a long format using ID 
  # (the only factor) as the categorical variable    
  LI_long <- melt(alltask) #version with outliers removed
  
  LI_long$ID<-rep(1:myrow,6)
  LI_long$Session<-c(rep(1,myrow*6),rep(2,myrow*6))
  long_task_list <- vector() # create an empty vector
  for (t in 1:6){
    long_task_list <- c(long_task_list, rep(myshortlab[t], myrow))
  }
  LI_long$Task<- long_task_list
  colnames(LI_long)[2]<-'LI'
  
 png(filename="LIpirateA2.png", width=1500, height=1200,res=300)

  # Make pirate plot
  pirateplot(formula = LI ~ Session + Task,
             data = LI_long,
             main = "",
             ylab = "LI Value",
             ylim = c(-3, 7),
             point.o=.5,inf.f.o=.1,inf.b.o=0,
             jitter.val=.075)
  
  abline(h=0,lty=2)
  
  xmarker_nums <- c(1,2,4,5,7,8,10,11,13,14,16,17) # For plotting x-axis
  # Mark significant differences (excluding outlier data) with black astrisks
  text(xmarker_nums, y=as.numeric(tdf[,3]), labels=tdf[,4],font=2, cex=1.3)
  
  # Mark outlier values as red dots
  # points(outlier_markers, y=outlier_values, type = "p", pch = 16, col="red")


 dev.off()
}

```

![Pirate plot](LIpirateA2.png)


##Correlation matrix as heatmap
```{r corrheat}

if (run_corrheat == 1){ # See run_corrheat toggle
  
  png(filename="heatmap.png", width=1500, height=1500,res=300)
  cormat<-round(cor(alltask[,ilist], use="pairwise.complete.obs", method="pearson"),2)
  rownames(cormat)<-c('A1','A2','B1','B2','C1','C2','D1','D2','E1','E2','F1','F2')
  colnames(cormat)<-c('A1','A2','B1','B2','C1','C2','D1','D2','E1','E2','F1','F2')
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
  upper_tri <- get_upper_tri(cormat)
  
  melted_cormat <- melt(upper_tri,na.rm=T)
  #for some reason NA values remain: need to remove these or they create grey cells
  #w<-which(is.na(melted_cormat[,3]))
  #melted_cormat<-melted_cormat[-w,]
  
  colnames(melted_cormat)[1:2]<-c('Var1','Var2') #names of cols 1 and 2
  # Create correlation heatmap using ggplot
  ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(colour = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(vjust = 1, 
                                     size = 12, hjust = 0.5),
          axis.text.y = element_text(vjust = 0.5, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  #add some formatting
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.5, 0.7),
      legend.direction = "horizontal")+
    guides(fill = guide_colorbar(barwidth = 9, barheight = 1.5,
                                 title.position = "top", title.hjust = 0.5))
  
  dev.off()
}
```

![Heatmap](heatmap.png)
##Structural equation Models: 

```{r setupbigsummary}
#We'll create a Table, bigsummary, to hold fit stats for different Models
bigsummary <- data.frame(matrix(NA,nrow=1,ncol=10))
bigsummary[1,2]<-0 #add a value so we recognise empty dataframe when adding first row
colnames(bigsummary)<-c('-2LL','df','CFI','RMSEA','Comparison','chi.diff','chi.df','p','BIC','Summary')

#Also important to clear previous models to avoid errors
#rm(Model1fit)
#rm(Model2fit)

```

```{r define.addbig}
#This function will add a row to bigsummary for each new Model
addbig <- function(thissummary,mycomp,thiscomment,thisrowname) {
  
  myLL<-round(thissummary$Minus2LogLikelihood,1)
  mydf<-thissummary$degreesOfFreedom
  mychi<-round(thissummary$Chi,1)
  mydfdiff<-thissummary$ChiDoF
  myp<-round(thissummary$p,3)
  if (myp < 0.001){myp<-'<.001'}
  if (myp==1){myp<-NA} #for first model
  myBIC<- round(thissummary$BIC.Mx,1)
  myCFI<-round(thissummary$CFI,3)
  myRMSEA<-round(thissummary$RMSEA,3)
  
  thisrow<-nrow(bigsummary)
  if(bigsummary[thisrow,2]>0){thisrow<-thisrow+1}
  bigsummary[thisrow,]<-c(myLL,mydf,myCFI,myRMSEA,mycomp,mychi,mydfdiff,myp,myBIC,thiscomment)
  rownames(bigsummary)[thisrow]<-thisrowname
  return(bigsummary)
}
```

###Model 1: Free Means and Vars Model 
(from fig 4 in prereg document)

This acts as baseline: it just Models means and variance: no relation between variables, 
and no consistency in LI over time.

We can then test how other Models fit when we introduce constraints by
equalising paths or by Modeling covariances.

N.B. I previously described this as a saturated Model, but the fully saturated Model includes covariances
(which are all zero) and so has more DF. The free means Model can be evaluated against the fully saturated one, using mxRefModels to specify the saturated Model. When that is done, we get rmsea and CFI provided for relative fit.
Start with Models of means

```{r freemeans_Model1}
# residual variances
dataRaw      <- mxData( observed=alltask, type="raw" )
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; free to vary by test and occasion
                        labels=c("e1","e2","e3","e4","e5","e6","e7","e8","e9","e10","e11","e12") )
# each has a different name, meaning they are estimated with different values
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =mylabels ) 
myModel1 <- mxModel("Free means Model", type="RAM",
                    manifestVars=mylabels,
                    dataRaw, resVars,  means)
Model1Fit <- mxRun(myModel1)
summary.Model1<-summary(Model1Fit, refModels=mxRefModels(Model1Fit, run = TRUE)) #gives CFI, TLI, RMSEA if refModels specified - we won't use these as they aren't meaningful here
summary.Model1
#Define labels to add to bigtable
summary.Model1$Chi<- NA
summary.Model1$ChiDoF<-  NA
summary.Model1$p<- 1
summary.Model1$RMSEA<- NA
summary.Model1$BIC.Mx<- NA
summary.Model1$CFI<- NA
mycomp<-'-'
thiscomment<-'Free means/vars'
thisrowname<-'1.Independent data'
bigsummary<-addbig(summary.Model1,mycomp,thiscomment,thisrowname) 
#Note that the Model has 24 parameters (i.e. 12 means and 12 vars), but the Saturated Model has 90, as it
#also includes all the covariances between 12 variables.

```

N.B. The summary output shows estimates of variances (e) and means (M). Note that the variance is particularly high for SentGen2. Also v low variance for ListGen1

###Model 2: Tweak free means Model to check stability of means
 Means and variances set to be the same for time1 and time2 for each measure. 
 N.B. for this model and all subsequent means models, OpenMx gives a warning about possible misspecification of the model. This is because covariances are omitted from the model, but are included in the reference model. I think this need not be a problem, as we are anticipating v poor model fit, precisely because we ignore covariances, when the correlation matrix makes it clear that we need to model them. The models here, though, are intended just to compare between different theories about means, and to compare models that all suffer from this same deficiency. 
 
 
 This is achieved by giving the path the same name, e.g. meanA for A1 and A2
 N.B. This is not same as test-retest reliability: as covariances not considered.
 We're just testing if the mean values are similar time 1 and time 2, not whether same people are high or low.
 

```{r Model2}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels = c(myshortlab,myshortlab) ) 

myModel2 <- mxModel("Stability Model", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model2Fit <- mxRun(myModel2)
summary.Model2<-summary(Model2Fit, refModels=mxRefModels(Model2Fit, run = TRUE)) 
summary.Model2

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2
test.1.2<-mxCompare(Model1Fit,Model2Fit)
summary.Model2$Chi<-test.1.2$diffLL[2]
summary.Model2$ChiDoF<-test.1.2$diffdf[2]
summary.Model2$p<-test.1.2$p[2]
mycomp<-'Model 1'
thiscomment<-'Equal means/vars'
thisrowname<-'2. Stable task effect'
bigsummary<-addbig(summary.Model2,mycomp,thiscomment,thisrowname) 

# Make a message (pmessage) that tells us what the Model comparison tells us. 
# Here we are looking for a nonsignificant p-value: that tells us that despite having fewer estimated parameters, fit does not suffer
# df is N observations (nsub*nvalues = 28*12) minus N estimated parameters (ep)
pmessage<-'Model 2 fit deteriorates relative to Model 1! Means differ across test occasions' #default message
if(test.1.2$p[2]>.05){
  pmessage <- 'Model 2 fit does not deteriorate relative to Model 1; ie means/vars equivalent across test occasions'}

pmessage

```
###Model 3: Model where all tests equivalent

More extreme version of stability Model, where all means and all vars are the same.
This tests (rather implausible!) hypothesis that all measures are similarly lateralised. It is equivalent to the 'population bias' Model.
We expect fit to worsen relative to stability Model - assuming measures differ in extent of laterality.
```{r Model3}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e1","e1","e1","e1","e1","e1","e1","e1","e1","e1","e1") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("meanA","meanA","meanA",
                                  "meanA","meanA","meanA","meanA","meanA","meanA",
                                  "meanA","meanA","meanA") ) 
myModel3 <- mxModel("Model3", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model3Fit <- mxRun(myModel3) #The mxRun command evaluates the Model.
summary.Model3<-summary(Model3Fit, refModels=mxRefModels(Model3Fit, run = TRUE)) 
summary.Model3

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2
#NB Model with most DF is last; Model 3 has many DF because no task-specific terms

test.2.3<-mxCompare(Model2Fit,Model3Fit)
summary.Model3$Chi<-test.2.3$diffLL[2]
summary.Model3$ChiDoF<-test.2.3$diffdf[2]
summary.Model3$p<-test.2.3$p[2]
mycomp<-'Model 2'
thiscomment<-'All means/vars equal'
thisrowname<-'3. Population bias'
bigsummary<-addbig(summary.Model3,mycomp,thiscomment,thisrowname) 

#We predict that means differ, in which case p will be < .05
pmessage<-'Means do not differ between tasks' #default
if(test.2.3$p[2]<.05){pmessage <- 'Means differ between tasks'}

pmessage

```
###Model 4: Test dorsal/ventral/mixed Model

This proposes pattern of means different for taskAB, C and DEF (Model 2a in prereg document).
Model predicts strength of laterality will be: AB > DEF > C.
We test this by equating means and variances for AB and DEF.

```{r dorsalventral}
# Sets order of tasks AB, DEF and C for Model 4 prediction
model4_resVars_order <- c("e1","e1","e2","e3","e3","e3","e1","e1","e2","e3","e3","e3")
model4_labels_order  <- c("meanAB","meanAB","meanC",
                                  "meanDEF","meanDEF","meanDEF",
                                  "meanAB","meanAB","meanC",
                                  "meanDEF","meanDEF","meanDEF")

# Changes order of tasks AB, DEF and C for Model 4 prediction if the tasks have been reordered
if (tryreorder == 1){ # See tryreorder toggle
  model4_resVars_order <- model4_resVars_order[nuorder]
  model4_labels_order <- model4_labels_order[nuorder]
}

# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels= model4_resVars_order)
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=TRUE, values=rep(1,12),
                        labels = model4_labels_order) 

myModel4 <- mxModel("Model4", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model4Fit <- mxRun(myModel4) #The mxRun command evaluates the Model.
summary.Model4<-summary(Model4Fit, refModels=mxRefModels(Model4Fit, run = TRUE)) 
summary.Model4

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary
test.2.4<-mxCompare(Model2Fit,Model4Fit)
summary.Model4$Chi<-test.2.4$diffLL[2]
summary.Model4$ChiDoF<-test.2.4$diffdf[2]
summary.Model4$p<-test.2.4$p[2]
mycomp<-'Model 2'
thiscomment<-'AB>DEF>C'
thisrowname<-'4.Dorsal/ventral stream'
bigsummary<-addbig(summary.Model4,mycomp,thiscomment,thisrowname) 

#Add some output showing the mean estimates and testing if they fit the expected pattern
my4<-summary(Model4Fit)$parameters[c(4,6,5),c(1,5,6)]
qmessage<-'Model predicts AB > DEF > C'
rmessage<-'Not confirmed'
if((my4[1,2]>my4[3,2])&&(my4[3,2]>my4[2,2])){rmessage<-'Confirmed'}

pmessage
qmessage
rmessage
my4

```
###Model 5: Lexical retrieval Model
Hypothesis A.2 Strength of lateralization depends on the extent to which tasks require lexical retrieval (more lexical retrieval = stronger left lateralization).

Operationalised by setting means equal for tasks BD, ACF and E (Model 5)

```{r lexical retrieval}
# Sets order of tasks BD, ACF and E for Model 5 prediction
model5_resVars_order <- c("e1","e2","e1","e2","e3","e2","e1","e2","e1","e2","e3","e2")
model5_labels_order  <- c("meanACF","meanBD","meanACF",
                          "meanBD","meanE","meanACF",
                          "meanACF","meanBD","meanACF",
                          "meanBD","meanE","meanACF")

# Changes order of tasks AB, DEF and C for Model 4 prediction if the tasks have been reordered
if (tryreorder == 1){ # See tryreorder toggle
  model5_resVars_order <- model5_resVars_order[nuorder]
  model5_labels_order <- model5_labels_order[nuorder]
}

# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=model5_resVars_order)
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =model5_labels_order) 

myModel5 <- mxModel("Model5", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model5Fit <- mxRun(myModel5) #The mxRun command evaluates the Model.
summary.Model5<-summary(Model5Fit, refModels=mxRefModels(Model5Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.2.5<-mxCompare(Model2Fit,Model5Fit)
summary.Model5$Chi<-test.2.5$diffLL[2]
summary.Model5$ChiDoF<-test.2.5$diffdf[2]
summary.Model5$p<-test.2.5$p[2]
summary.Model5

mycomp<-'Model 2'
thiscomment<-'BD>ACF'
thisrowname<-'5.LexicalRetrieval'
bigsummary<-addbig(summary.Model5,mycomp,thiscomment,thisrowname) 


#Add some output showing the mean estimates and testing if they fit the expected pattern
my5<-summary(Model5Fit)$parameters[c(5,4,6),c(1,5,6)] #pull out just the rows/cols of interest
qmessage<-'Model predicts BD > ACF'
rmessage<-'Not confirmed'
if(my5[1,2]>my5[2,2]){rmessage<-'Confirmed'}

pmessage
qmessage
rmessage
my5

```



##SEM Models including covariances
###Model 6: Single factor Model (means equalized for T1 and T2).

If tasks are correlated (as they are) this will give a better fit than the preceding Models, which only looks at means.
It is necessary to scale the paths from observed variables to latent factor in terms of one of the observed variables. We chose the first variable. NB I have checked that choice does not affect the factor solution.

```{r Factor1}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )

# latent variance - Factor1 is the single factor
latVar       <- mxPath( from="Factor1", arrows=2,
                        free=T, values=1, labels ="varFactor1" )

# factor loadings
facLoads     <- mxPath( from="Factor1", to=mylabels, arrows=1,
                        free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                        values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("l1","l2","l3","l4","l5","l6","l1","l2","l3","l4","l5","l6") )#same for each test on time 1 and 2

#The first path is fixed at one - others scaled relative to this

# means - one extra mean for the Factor, but this is set to NA
means        <- mxPath( from="one", to=c(mylabels,'Factor1'), arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T,FALSE), values=c(1,1,1,1,1,1,1,1,1,1,1,1,0),
                        labels =c(myshortlab, myshortlab, NA) ) #means constant from time 1 to time 2

myModel6 <- mxModel("Single Factor Model", type="RAM",
                          manifestVars=mylabels, latentVars="Factor1",
                          dataRaw, resVars, latVar, facLoads, means)

Model6Fit <- mxRun(myModel6)
summary.Model6<-summary(Model6Fit, refModels=mxRefModels(Model6Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.6.2<-mxCompare(Model6Fit,Model2Fit)
summary.Model6$Chi<-test.6.2$diffLL[2]
summary.Model6$ChiDoF<-test.6.2$diffdf[2]
summary.Model6$p<-test.6.2$p[2]
summary.Model6

mycomp<-'Model 2'
thiscomment<-'Covariances: One factor'
thisrowname<-'6.Person Effect'
bigsummary<-addbig(summary.Model6,mycomp,thiscomment,thisrowname) 


pmessage<-'One factor Model no better than Model with no covariance between measures'
if(test.6.2$p[2]<.05){
  pmessage <- paste0('One factor Model is better fit than Model with no covariance between measures')
}
pmessage
bigsummary

```
Note how the CFI is now in a more sensible range, but still is well below acceptable level.
Will a bifactor Model improve this?

###Model 7: Bifactor Model
Here we specify two independent latent factors. Aim is to see whether the tests form two clusters.

```{r bifactor}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )

# latent variances and covariance: NB assume totally independent, so covariance fixed at zero
latVars      <- mxPath( from=c("Factor1","Factor2"), arrows=2, connect="unique.pairs",
                        free=c(T,F,F), values=c(1,0,1), labels=c("varFactor1","cov","varFactor2") )

#changed the free statement from free =c(T,T,T) (that was error in prereg script: gives underidentified Model)

# factor loadings for Factor1 #NB test A loading is fixed to one for this factor
facLoadsFactor1     <- mxPath( from="Factor1", to=mylabels, arrows=1,
                               free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                               values=rep(1,12),
                               labels =c("k1","k2","k3","k4","k5","k6","k1","k2","k3","k4","k5","k6") )

# factor loadings for Factor2 #NB test A loading is fixed to zero for this factor
facLoadsFactor2     <- mxPath( from="Factor2", to=mylabels, arrows=1,
                               free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                               values=c(0,rep(1,5),0,rep(1,5)),
                               labels =c("l1","l2","l3","l4","l5","l6","l1","l2","l3","l4","l5","l6") )

# means #estimated for all except the two factors
means        <- mxPath( from="one", to=c(mylabels,'Factor1','Factor2'), arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T,FALSE,FALSE), values=c(1,1,1,1,1,1,1,1,1,1,1,1,0,0),
                        labels =c(myshortlab, myshortlab, NA,NA) )

myModel7 <- mxModel("BiFactor Model", type="RAM",
                          manifestVars=mylabels,
                          latentVars=c("Factor1","Factor2"),
                          dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)

Model7Fit <- mxRun(myModel7)
summary.Model7<-summary(Model7Fit, refModels=mxRefModels(Model7Fit, run = TRUE)) 
summary.Model7

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.7.6<-mxCompare(Model7Fit,Model6Fit)
summary.Model7$Chi<-test.7.6$diffLL[2]
summary.Model7$ChiDoF<-test.7.6$diffdf[2]
summary.Model7$p<-test.7.6$p[2]
mycomp<-'Model 6'
thiscomment<-'Covariances: bifactor'
thisrowname<-'7.Task x Person Effect'
bigsummary<-addbig(summary.Model7,mycomp,thiscomment,thisrowname) 

pmessage<-'Bi-factor Model does not improve fit over one factor Model'

if (test.7.6$p[2]<.05){pmessage <- paste0('Bi-factor Model is better fit than one factor Model' )}
pmessage
test.7.6
bigsummary
#mxCheckIdentification(myModel7, details=TRUE) # check Model identification: all OK

```
Better fit for bifactor than for single factor, though CFI is still below accepted value for good fit. 
To explore reasons for poor fit, can look at differences between expected and observed covariances.



###Drop NS paths from bifactor
```{r pathdrop}
if (run_pathdrop == 1){ #See run_pathdrop toggle
  npaths<-length(mylabels)-2 # ten labels (as two paths are fixed)
  nvars<-length(mylabels)/2  # six variables (tasks)
  
  # Identifies path estimates and standard errors to caclulate z scores
  mypaths<-summary.Model7$parameters$Estimate[1:npaths]
  mySEs<-summary.Model7$parameters$Std.Error[1:npaths]
  myzs<-mypaths/mySEs
  #insert dummy value of 3 for the fixed paths to insure pth index is correct (there should be 12 paths in myzs)
  myzs<-c(3,myzs[1:(npaths/2)],3,myzs[((1+npaths/2):npaths)])
  
  # Identify paths where z is not significant (<1.65)
  w1<-which(myzs[1:nvars]<1.65) #paths to Factor1
  w2<-which(myzs[(1+nvars):(2*nvars)]<1.65) #paths to Factor2
  
  # Default is that all paths should be estimated (TRUE) except the fixed ones
  myfreef1<-c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE)
  myfreef2<-c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE)
  
  myvalsf1<-rep(1,12)                 # In factor 1, the values are all set to 1, but the free parameters are then estimated
  myvalsf2<-c(0,rep(1,5),0,rep(1,5))  # In factor 2, the fixed parameters are set to 0 instead of 1
  
  mylabsf1<-c("k1","k2","k3","k4","k5","k6","k1","k2","k3","k4","k5","k6") 
  mylabsf2<-c("l1","l2","l3","l4","l5","l6","l1","l2","l3","l4","l5","l6") 
  
  # If there are any non-significant paths, they are removed
  if(length(w1)>0){
    myfreef1[w1]<-FALSE
    myvalsf1[w1]<-0
    myfreef1<-rep(myfreef1[1:nvars],2)
    myvalsf1<-rep(myvalsf1[1:nvars],2)
  }
  
  if(length(w2)>0){
    myfreef2[w2]<-FALSE
    myvalsf2[w2]<-0
    myfreef2<-rep(myfreef2[1:nvars],2)
    myvalsf2<-rep(myvalsf2[1:nvars],2)
  }
  
  # change factor loadings accordingly
  facLoadsFactor1     <- mxPath( from="Factor1", to=mylabels, arrows=1,
                                 free=myfreef1, 
                                 values=myvalsf1,
                                 labels =mylabsf1)
  
  # factor loadings for Factor2 #NB test A loading is fixed to zero for this factor
  facLoadsFactor2     <- mxPath( from="Factor2", to=mylabels, arrows=1,
                                 free=myfreef2, 
                                 values=myvalsf2,
                                 labels =mylabsf2) 
  
  myModel7drop <- mxModel("BiFactor Model drop", type="RAM",
                          manifestVars=mylabels,
                          latentVars=c("Factor1","Factor2"),
                          dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)
  
  Model7dropFit <- mxRun(myModel7drop)
  
  summary.Model7drop<-summary(Model7dropFit, refModels=mxRefModels(Model7dropFit, run = TRUE)) 
  summary.Model7drop
  
  #to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary
  
  test.7d.7<-mxCompare(Model7Fit,Model7dropFit)
  summary.Model7drop$Chi<-test.7d.7$diffLL[2]
  summary.Model7drop$ChiDoF<-test.7d.7$diffdf[2]
  summary.Model7drop$p<-test.7d.7$p[2]
  mycomp<-'Model 7'
  thiscomment<-'Covariances: bifactor dropns'
  thisrowname<-'7.Task x Person Effect drop ns'
  bigsummary<-addbig(summary.Model7drop,mycomp,thiscomment,thisrowname) 
}

```
### Save bigsummary table
```{R writebigsummary}
 write.table(bigsummary,paste0('bigsummary_fixed_',myfixed,'.txt'), sep="\t",row.names=FALSE,quote=FALSE)
```

##All subsequent analyses explore characteristic of a specific model, which should be specified here

```{r selectmodel}
thissummary<-summary.Model7drop
thisfit<-Model7dropFit
thismodel<-myModel7drop
```
### Explore covariance matrix vs expected
We can extract the observed covariance matrix and compare it with the expected values from the current Model. If we take the difference between these, we can identify whether there are particular covariances that are problematic.
```{r covexplorefunction}
covexplore<-function(ModelFit,mytitle){
  
  mymat.e<-mxGetExpected(ModelFit,"covariance")
  mymat.o <- cov(alltask[,1:12],use="pairwise.complete.obs")
  mymat.d<-abs(mymat.e-mymat.o) #absolute size of mismatch between obs and expected
  mymat.d1<-mymat.e-mymat.o #size of mismatch with sign
  cc<-rainbow(ncol(mymat.d1))
  h<-heatmap(mymat.d1,keep.dendro=FALSE,Rowv=NA,Colv=NA,
             revC=TRUE,col = cm.colors(256),margins=c(5,5),
             main=mytitle)
  return(h)
}
```

```{r makeheatmap}
if (run_covexplore == 1){
  covexplore(thisfit,'Model 7drop: obs/exp covariance diffs')
}
```

Dark colours (pink or blue) indicate lack of agreement between obs and expected (neg or positive). 

### Draw simplified path diagram
Draw diagram of the selected model, with nonsignificant paths omitted.
The criterion for a non-significant path is currently set to z = 1.65, but it is rather arbitrary, and the picture will look different depending on how stringent you make it.
N.B. The file for_graphviz is set up in advance and read in and modified according to results. For this part of the script to work you must have the file 'for_graphviz.csv' in your working directory.

Also, if this file is knitted as a word document, the figure won't render. To see it, knit document as html.

Test A is shown in red as this has fixed paths to X1 (1) and X2 (0).

NB If this figure is published, need to make following points:

* This is simplified path diagram. It shows just one measure per variable, when in fact there were two, and it does not show means, though these were estimated.

* Also nonsignificant paths are omitted.


```{r insertblanks}
#For graphviz to work as set up, and when running jacknife, need to add dummy rows for dropped paths

insertblanks<-function(thissummary){
  
  ms<-which(thissummary$parameters$matrix=='A')
  startsummary<-thissummary$parameters[ms,c(1,3:6)]
  startsummary$z<-startsummary$Estimate/startsummary$Std.Error
  
  namelist<-thissummary$parameters$name[ms]
  fullnamelist<-c('k2','k3','k4','k5','k6','l2','l3','l4','l5','l6')
  w<-which(fullnamelist %in% namelist) 
  
  mysummary<-data.frame(matrix(NA,nrow=length(fullnamelist),ncol=dim(startsummary)))#create a clone 
  colnames(mysummary)<-colnames(startsummary)
  mysummary$name<-fullnamelist
  mysummary$z<-0 #initialise with zero
  for (i in 1:length(w)){
    mysummary[w[i],]<-startsummary[i,]
  }
  return(mysummary)
}
```

```{r drawpaths}
if (run_drawpaths == 1){
  
  # omxGraphviz(biFactorModel, dotFilename = "bifactor.dot")
  # grViz("bifactor.dot") #this will generate a .dot file but it
  # is messy, as it shows time 1 and time 2 measures, as well as means
  
  thatsummary<-insertblanks(thissummary)
  # Script below shows time1/time2 combined and omits means for clarity
  mybit<-read.csv('for_graphviz.csv',stringsAsFactors = FALSE,header=FALSE) #full list of all paths.
  mybit2<-print.data.frame(mybit, 
                           quote=FALSE) #get rid of quotes
  
  #we now want to a) remove rows that are NS and b) put in path coeffs for the rest
  thisrow<-12 #NB: first row with path specification is col 13
  thatrow<-0 #counter for the summary z scores: NB these exclude measure A! 
  for (j in 1:2){#each *factor* (not each test occasion - these are collapsed in diagram)
    
    for (i in 1:6){ #each task 
      thisrow<-thisrow+1
      
      if(i>1){ #measure A is fixed so not in the table
        thatrow<-thatrow+1
        if(thatsummary$z[thatrow]<1.65)
        {mybit2[thisrow,]<-''} #delete this one
        else{
          pathlabel<-round(thatsummary$Estimate[thatrow],2)
          bb<-mybit2[thisrow,]
          bb<-str_replace(bb,'xx',as.character(pathlabel))
          mybit2[thisrow,]<-bb
        }
      } #loop to here when i is 1: no action
    }
  }
  mybit2[19,]<-'' #delete path for A to X2: this one was fixed to 0
  graphname<-paste0(colnames(alltask)[1],'_',colnames(alltask)[2],'_',colnames(alltask)[3],'_',colnames(alltask)[4],'_',colnames(alltask)[5],'_',colnames(alltask)[6])
  mybit2[25,]<-paste("label=",graphname,";")
  
  dotFilename<-'dottry.dot'
  if(usemean==0){
    dotFilename<-'dottry_peak.dot'
  }
  
  
  write.table(mybit2, dotFilename, append = FALSE,
              row.names = FALSE, col.names = FALSE,quote=FALSE)
  
  grViz(dotFilename)
  # pathplot<-DiagrammeR::grViz(dotFilename)
  # pathplotname<-paste0('pathplot_fix_',myfixed,'.png')
  # pathplot %>% export_svg %>% charToRaw %>% rsvg_png(pathplotname)
}
 
```


##Check solution robustness with drop one approach and explore results

Final bifactor model re-run with one subject dropped each time.
Standardized factor loading for each run saved in dropone_z.csv.
###Runs dropone analysis
```{r dropone, echo=FALSE}

if (run_dropone==1){
  #First create dataframe to save the results
  droponesummary<-insertblanks(thissummary)
  nest<-1:(nrow(droponesummary)/2) #range for ests for F1
  nest2<-length(nest)+nest #range for ests for F2
  droponesummary$row<-rep(myshortlab[1+nest],2)
  droponesummary$col[nest]<-'Factor1'
  droponesummary$col[nest2]<-'Factor2'
  
  #Use'drop one' approach - reduce the dataset by one on each run
  mycols<-nrow(summary(thisfit)$parameters)
  PT_summary<-matrix(NA,ncol=mycols, nrow=myrow) #initialise matrix to gather estimates for each of the parameters for each of 30 runs
  
  for (thisdrop in 1:myrow){
    dataRaw      <- mxData( observed=alltask[-thisdrop,], type="raw" )
    
    #keep all other parameters same as last model that was run
    
    thismodel<- mxModel("BiFactor Model", type="RAM",
                        manifestVars=mylabels,
                        latentVars=c("Factor1","Factor2"),
                        dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)
    currentFit <- mxRun(thismodel)
    droppedsummary<-summary(currentFit)
    
    PT_summary[thisdrop,]<-droppedsummary$parameters$Estimate #nb all estimates - not just paths
    
    temp<-insertblanks(droppedsummary) #use previously defined function to give null values for dropped paths
    nucol<-ncol(droponesummary)+1
    droponesummary[,nucol]<-round(temp$z,2)
    colnames(droponesummary)[nucol]<-paste0('z.',thisdrop)
  }
  myfixed<-myshortlab[1]
  dropname<-paste0('dropone_z_',myfixed,'.txt')
  write.table(t(droponesummary), dropname, sep="\t",quote=F) 
  t(droponesummary) #display transposed
}
```

###Make factor scores and plot them
In a further exploratory analysis, we plotted factor scores by making a weighted sum of scores, using factor loadings as weights.
This shows that although the factors are modeled to be independent, they are highly intercorrelated. 

```{r makefacscores}
if (run_facscores == 1 & run_dropone == 1){ # See run_facscores toggle
  
  #? Use loadings or zscores?
  #(Nb updated 28/8/18 to include measures from both time points)
  #This is with loadings - but it makes little difference
  #In weighted sum use original data, including excluded datapoints (alltaskall) to avoid losing subjects from this
  allz1<-c(1,thatsummary$z[1:5])
  allz2<-c(0,thatsummary$z[6:10]) #1st and 6th values for task A
  for (i in 1:nrow(alltask)){
    alltask$f1score[i]<-alltaskall[i,1]*allz1[1]+
      alltaskall[i,2]*allz1[2]+
      alltaskall[i,3]*allz1[3]+
      alltaskall[i,4]*allz1[4]+
      alltaskall[i,5]*allz1[5]+
      alltaskall[i,6]*allz1[6]+
      alltaskall[i,7]*allz1[1]+
      alltaskall[i,8]*allz1[2]+
      alltaskall[i,9]*allz1[3]+
      alltaskall[i,10]*allz1[4]+
      alltaskall[i,11]*allz1[5]+
      alltaskall[i,12]*allz1[6]
    
    alltask$f2score[i]<-alltaskall[i,1]*allz2[1]+
      alltaskall[i,2]*allz2[2]+
      alltaskall[i,3]*allz2[3]+
      alltaskall[i,4]*allz2[4]+
      alltaskall[i,5]*allz2[5]+
      alltaskall[i,6]*allz2[6]+
      alltaskall[i,7]*allz2[1]+
      alltaskall[i,8]*allz2[2]+
      alltaskall[i,9]*allz2[3]+
      alltaskall[i,10]*allz2[4]+
      alltaskall[i,11]*allz2[5]+
      alltaskall[i,12]*allz2[6]
  }
  
  pngname<-paste0('factorplot_fix',myfixed,'.png')
  png(pngname, width=4, height=4, units="in", res=300)
  plot(alltask$f1score,alltask$f2score,pch=15)
  abline(v=0,lty=2)
  abline(h=0,lty=2)
  mycor<-round(cor(alltask$f1score,alltask$f2score,use='pairwise.complete.obs'),3)
  text(-20,15,paste('r = ',mycor))
  dev.off()
}
```

###Jacknife estimates of path SEs
Jacknife estimates considered for model SEs, which can be compared with standard method. The jacknife method uses the output from the drop-one analysis to look at distribution of estimates obtained with one subject omitted.
It confirms that there are particular problems with estimates of paths from Jabberwocky to Factor 1 and from Sentence Generation to Factor 2.

```{R jackknife}
if (run_dropone == 1){ # Only runs if dropone has also run
  # Calculates the Jackknife estimates (bias corrected) and the bias. Then prints out corrected estimates vs the "real" estimates from the original model 8 run.

 n=myrow
 real_est<-thissummary$parameters$Estimate #estimate with all cases included
 #PT_summary
 PT_means<-colMeans(PT_summary) #average estimate across all dropone runs
 JK_bias<-(n-1)*(PT_means-real_est) 
 
 Jack_est<-(n*real_est)-((n-1)*PT_means)
pngname<-paste0('jacknifeplot_fix',myfixed,'.png')
png(pngname, width=4, height=4, units="in", res=300)

 plot(real_est,Jack_est)
 dev.off()
 
 # calculates the Jackknife SE estimates to allow us to calculate the CI.
 
 #PT_summary
 mycol<-length(thissummary$parameters$Estimate)
 Jack_SE<-vector(mode="numeric",length=mycol)
 for (i in 1:(mycol))
 {
 Jack_SE[i]<-sqrt(((n-1)/n)*sum((PT_summary[,i]-PT_means[i])^2))
 }
 myres<-thissummary$parameters[,1:6]
 myres$L_ci<-round(thissummary$parameters$Estimate-1.65*thissummary$parameters$Std.Error,2)
 myres$U_ci<-round(thissummary$parameters$Estimate+1.65*thissummary$parameters$Std.Error,2)
 myres$Jack_Estimate<-round(Jack_est,2)
 myres$Jack_Std.Error<-round(Jack_SE,2)
 myres$JK_L_ci<-round(Jack_est-1.65*Jack_SE,2)
 myres$JK_U_ci<-round(Jack_est+1.65*Jack_SE,2)
 myres$JK_bias<-round(JK_bias,2)
 myres$Estimate<-round(myres$Estimate,2)
 
 myres[,c(1,3,4,5,9)]

 write.csv(myres,"Jacknife_ests.csv")
}
```

##Session information
```{r sessinfo}
sessionInfo()
```