---
title: "Analysis for A2 writeup"
author: "Dorothy Bishop/ Zoe Woodhead / Paul Thompson"
date: "28 Jul 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

`r Sys.time()`

## Background
Original protocol for project is here: <https://osf.io/tkpm2/registrations/>

See Dopp_explore_log.RMd for more background on how Modeling was developed.

Source reference is: Kline, R. B. (2011). Principles and practice of structural equation Modeling, 3rd edition. New York: Guilford Press.
Kline has detailed discussion of testing for fit: is against absolute cutoffs, though notes cutoffs that have been proposed on basis of simulation studies.

RMSEA is absolute index of (badness of) Model fit: aim for below .08.
This value decreases with more DF (greater parsimony) or larger sample size.
 
Comparative fit index (CFI) measures relative improvement in fit of Model over baseline Model. 
Computed as 1 - (chisqM - dfM)/(chisqB-dfB); Good fit if CFI > .95 (see Kline p 208)

##Chunks of code for setting packages and reading in data 
from SEM2_maxLikelihood_realdata.R
```{r packages, warning=FALSE, message=FALSE}
#Needs OpenMx, which you get with following command (not CRAN)
#Hmm - having problems with OpenMx - now using Cran version

#source('https://openmx.ssri.psu.edu/software/getOpenMx.R')
require(tidyverse)
require(OpenMx)
require(stargazer) #simple commands for nice tables
require(semTools) #for fit measures
library(DiagrammeR) #for the diagram
require(stringr)
require(data.table)
library('devtools')
library("reshape2")
library("yarrr")


```
##Code for setting directory flexibly
```{r definedir}
#This not currently used but should be easy to adapt if needed. 
#recommend just setting working directory to current folder where files reside
thisuser <-'dorothybishop'
thisdrive<- '' #may need to set this to 'C:' for PC users
#readdir <-paste0(thisdrive,'/Users/',thisuser,'/Dropbox/project A2/')
readdir<-'' #use this if working directory set to folder with all the data
```

```{r participants}
particdat<-read.csv('A2_Participant_Info.csv')

#Participant descriptors reported in paper
print('Gender x handedness')
table(particdat$Gender,particdat$handedness)
ageyr<-particdat$Age_m/12
print('Age in yr')
summary(ageyr)
```

##Task performance data
```{r behaviouraldata}
behavdat<-read.csv('A2_Behavioural_Data.csv')
behavdat<-behavdat[1:30,] #remove some blank lines at end of file
stargazer(behavdat[,2:5],type='text')

acc.cols<-c(7,10,13,16,19,22,25,28)
rt.cols<-acc.cols+1
#accuracy data on tasks B,C, E and F - stargazer for quick check
print('Percentages correct')
stargazer(behavdat[,acc.cols],type='text')
print('RT')
stargazer(behavdat[,rt.cols],type='text')

#make data frame to hold data formatted for paper
behavdf<-data.frame(matrix(NA,nrow=4,ncol=8))
colnames(behavdf)<-c('%corr.1','%corr.2','t','p','RT.1','RT.2','t','p')
#get means and SDs for table
mymeanp<-round(sapply(behavdat[,acc.cols],mean,na.rm=TRUE),1)
mysdp<-round(sapply(behavdat[,acc.cols],sd,na.rm=TRUE),2)
mymeanr<-round(sapply(behavdat[,rt.cols],mean,na.rm=TRUE),2)
mysdr<-round(sapply(behavdat[,rt.cols],sd,na.rm=TRUE),2)
for (i in 1:4){
  
behavdf[i,1]<-paste0(mymeanp[i],' (',mysdp[i],')')
behavdf[i,2]<-paste0(mymeanp[(i+4)],' (',mysdp[i+4],')')
behavdf[i,5]<-paste0(mymeanr[i],' (',mysdr[i],')')
behavdf[i,6]<-paste0(mymeanr[(i+4)],' (',mysdr[i+4],')')
thist<-t.test(behavdat[,acc.cols[i]],behavdat[,acc.cols[i+4]],paired=TRUE)
behavdf[i,3]<-round(thist$statistic,2)
behavdf[i,4]<-round(thist$p.value,3)
thist2<-t.test(behavdat[,rt.cols[i]],behavdat[,rt.cols[i+4]],paired=TRUE)
behavdf[i,7]<-round(thist2$statistic,2)
behavdf[i,8]<-round(thist2$p.value,3)
}
rownames(behavdf)<-c('B','C','E','F')
write.table(behavdf, "Behavtable.txt", sep="\t",row.names=TRUE,quote=FALSE) #Currently this is table 1
```
##Code for reading and reshaping LI data
N.B. Can select to use original LI values based on peak, or LI values from mean in POI.
```{r readdata}
#read in data from sessions 1 and 2, having saved as .csv
#NB. Need to set working directory to location of data files - or else specify path
usemean <-1 #set to zero to analyse original peak-based LI values
data1<-read.csv('Results_Session1.csv')
data2<-read.csv('Results_Session2.csv')

alltaskall <- cbind(select(data1,A1.LI,B1.LI,C1.LI,D1.LI,E1.LI,F1.LI),
             select(data2,A2.LI,B2.LI,C2.LI,D2.LI,E2.LI,F2.LI))

if(usemean==1){
alltaskall <- cbind(select(data1,A1.LI_mean,B1.LI_mean,C1.LI_mean,D1.LI_mean,E1.LI_mean,F1.LI_mean),
             select(data2,A2.LI_mean,B2.LI_mean,C2.LI_mean,D2.LI_mean,E2.LI_mean,F2.LI_mean))
}
mylabels<-c('ListGen1','PhonDec1','SemDec1','SentGen1','SentComp1','Jabber1',
            'ListGen2','PhonDec2','SemDec2','SentGen2','SentComp2','Jabber2')
colnames(alltaskall)<-mylabels
head(alltaskall)
```



##Code for identifying exclusions and substituting NA based on SE
Please see justification in Methods. This differs from what we preregistered but we can defend it as making more sense. I.e. we want to exclude runs where the data are unreliable, but we do not want to exclude runs where the LI is statistically unusual.

```{r excludeSE}
alltask<-alltaskall #create a new copy where the outliers will be coded as NA
#This bit of code selects just the R-handers
#w<-which(particdat$handedness=='R')
#alltask<-alltaskall[w,]
allse <- cbind(select(data1,A1.se,B1.se,C1.se,D1.se,E1.se,F1.se),
             select(data2,A2.se,B2.se,C2.se,D2.se,E2.se,F2.se))
if (usemean==1){
allse <- cbind(select(data1,A1.mean_se,B1.mean_se,C1.mean_se,D1.mean_se,E1.mean_se,F1.mean_se),
             select(data2,A2.mean_se,B2.mean_se,C2.mean_se,D2.mean_se,E2.mean_se,F2.mean_se))
}
myse<-c(allse[,1],allse[,2],allse[,3],allse[,4],allse[,5],allse[,6],
        allse[,7],allse[,8],allse[,9],allse[,10],allse[,11],allse[,12])

Q3<-quantile(myse,.75)
Q1<-quantile(myse,.25)
Qlimit<-Q3+2.2*(Q3-Q1)
secols<-colnames(allse)
dropSE<-0 #initialise counter
for (i in 1:12){
w<-which(allse[,i]>Qlimit)
if (length(w)>0){
alltask[w,i]<-NA
dropSE<-dropSE+1
}
}
print(paste('Dropped because high SE, N = ',dropSE))
#if usemean==1, picks up subject 5 on PHondec1, and subject 9 on Jabber1

#Now remove those with fewer than 12 trials in a condition
dropN<-0 #initialise counter
nbit<-c('A1.N','B1.N','C1.N','D1.N','E1.N','F1.N')
for (i in 1:6){
  w<-which(colnames(data1)==nbit[i])
  ww<-which(data1[,w]<12)
  if(length(ww>0)){
    alltask[ww,i]<-NA
    dropN<-dropN+1
  }
}

nbit<-c('A2.N','B2.N','C2.N','D2.N','E2.N','F2.N')
for (i in 1:6){
  w<-which(colnames(data2)==nbit[i])
  ww<-which(data2[,w]<12)
  if(length(ww>0)){
    alltask[ww,(i+6)]<-NA
     dropN<-dropN+1
  }
}
print(paste('Dropped because < 12 trials, N = ',dropN))  
  
#When usemean==1, dropped data points are S13 Sentgen2, S14 Listgen2, S26 Sentcomp1 and S30 Sentgen1
```
##Show means etc for tasks with time1 and time2 adjacent
```{r stargaze}
stargazer(alltask[,c(1,7,2,8,3,9,4,10,5,11,6,12)],type='text')
```
##Check normality of data
Have a look at the densities and do Shapiro-Wilks test and QQ plot
 Plots will be written to the working directory, called densities 1-6
```{r normalcheck}
for (i in 1:6){
 # png(filename=paste0("densities_",i,".png"))
  par(mfrow=c(2,2))
  for (j in 1:2){
    offset<- 6*(j-1)
  tempdata<-alltask[,(i+offset)]
  myshap<-shapiro.test(tempdata)
   plot(density(tempdata,na.rm=TRUE),main=mylabels[(i+offset)],xlim=c(-6,8),ylim=c(0,.3))
   text(-4,.1,paste('mean = \n',round(mean(tempdata,na.rm=TRUE),2)))
## Plot using a qqplot
   qqnorm(tempdata);qqline(tempdata, col = 2)
   text(.4,0,paste('Shapiro\np = ',round(myshap$p.value,3)))
  }
 # dev.off()
}
```
##Pirateplot

```{r dopirate}

#First do one sample t-tests to check if sig different from zero
tdf<-data.frame(matrix(NA,nrow=12,ncol=5))
colnames(tdf)<-c('t','p','labelht','label','task')
ilist<-c(1,7,2,8,3,9,4,10,5,11,6,12)
for (j in 1:12){
  i<-ilist[j]
  myt<-t.test(alltask[,i])
  tdf[j,1]<-round(myt$statistic,2)
  tdf[j,2]<-round(myt$p.value,3)
  tdf[j,3]<-max(alltask[,i],na.rm=TRUE)+1
  tdf[j,4]<-''
  if(myt$p.value<.05){tdf[j,4]<-"*"}
    if(myt$p.value<.01){tdf[j,4]<-"**"}
    if(myt$p.value<.001){tdf[j,4]<-"***"}
  tdf[j,5]<-colnames(alltask)[i]
}

# Melt cleverly reshapes LI_data into a long format using ID 
# (the only factor) as the categorical variable    
LI_long <- melt(alltask) #version with outliers removed
LI_long$ID<-rep(1:30,6)
LI_long$Session<-c(rep(1,180),rep(2,180))
LI_long$Task<-c(rep('A',30),rep('B',30),rep('C',30),rep('D',30),rep('E',30),rep('F',30),
                rep('A',30),rep('B',30),rep('C',30),rep('D',30),rep('E',30),rep('F',30)) 
colnames(LI_long)[2]<-'LI'
png(filename="LIpirateA2.png", width=1500, height=1200,res=300)
#par(mai = c(.5, .8, .05, 0.05))# mai specifies bottom, left, top and right margins in inches

# Make pirate plot
pirateplot(formula = LI ~ Session + Task,
           data = LI_long,
           main = "",
           ylim = c(-5, 8),
           point.o=.5,inf.f.o=.1,inf.b.o=0,
           jitter.val=.075)

 abline(h=0,lty=2)
 
 xmarker_nums <- c(1,2,4,5,7,8,10,11,13,14,16,17) # For plotting x-axis
 
# Mark significant differences from one sample ttest with black asterisks
text(xmarker_nums, y=as.numeric(tdf[,3]), labels=tdf[,4],font=2, cex=1)
 dev.off()


```

![Pirate plot](LIpirateA2.png)


#Correlation matrix as heatmap
```{r corrheat}

png(filename="heatmap.png", width=1500, height=1500,res=300)
cormat<-round(cor(alltask[,ilist], use="pairwise.complete.obs", method="pearson"),2)
rownames(cormat)<-c('A1','A2','B1','B2','C1','C2','D1','D2','E1','E2','F1','F2')
colnames(cormat)<-c('A1','A2','B1','B2','C1','C2','D1','D2','E1','E2','F1','F2')
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)

melted_cormat <- melt(upper_tri,na.rm=T)
#for some reason NA values remain: need to remove these or they create grey cells
#w<-which(is.na(melted_cormat[,3]))
#melted_cormat<-melted_cormat[-w,]

colnames(melted_cormat)[1:2]<-c('Var1','Var2') #names of cols 1 and 2
# Create correlation heatmap using ggplot
ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(colour = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(vjust = 1, 
                                   size = 12, hjust = 0.5),
        axis.text.y = element_text(vjust = 0.5, 
                                   size = 12, hjust = 1))+
  coord_fixed()

#add some formatting
ggheatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.5, 0.7),
    legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 9, barheight = 1.5,
                               title.position = "top", title.hjust = 0.5))

dev.off()
```

![Heatmap](heatmap.png)
##Structural equation Models: 


###Model 1: Free Means and Vars Model 
(from fig 4 in prereg document)

This acts as baseline: it just Models means and variance: no relation between variables, 
and no consistency in LI over time.

We can then test how other Models fit when we introduce constraints by
equalising paths or by Modeling covariances.

N.B. I previously described this as a saturated Model, but the fully saturated Model includes covariances
(which are all zero) and so has more DF. The free means Model can be evaluated against the fully saturated one, using mxRefModels to specify the saturated Model. When that is done, we get rmsea and CFI provided for relative fit.

```{r setupbigsummary}
#We'll create a Table, bigsummary, to hold fit stats for different Models
bigsummary <- data.frame(matrix(NA,nrow=1,ncol=10))
bigsummary[1,2]<-0 #add a value so we recognise empty dataframe when adding first row
colnames(bigsummary)<-c('-2LL','df','CFI','RMSEA','Comparison','chi.diff','chi.df','p','BIC','Summary')
  #Also important to clear previous models to avoid errors
 rm(Model1fit)
  rm(Model2fit)
# rm(biFactorFit)
# rm(biFactorFitb)
# rm(oneFactorFit)
```

```{r define.addbig}
#This function will add a row to bigsummary for each new Model
addbig <- function(thissummary,mycomp,thiscomment,thisrowname) {
  
myLL<-round(thissummary$Minus2LogLikelihood,1)
mydf<-thissummary$degreesOfFreedom
mychi<-round(thissummary$Chi,1)
mydfdiff<-thissummary$ChiDoF
myp<-round(thissummary$p,3)
if (myp<.001){myp<-'<.001'}
if (myp==1){myp<-NA} #for first model
myBIC<- round(thissummary$BIC.Mx,1)
myCFI<-round(thissummary$CFI,3)
myRMSEA<-round(thissummary$RMSEA,3)

thisrow<-nrow(bigsummary)
if(bigsummary[thisrow,2]>0){thisrow<-thisrow+1}
bigsummary[thisrow,]<-c(myLL,mydf,myCFI,myRMSEA,mycomp,mychi,mydfdiff,myp,myBIC,thiscomment)
rownames(bigsummary)[thisrow]<-thisrowname
return(bigsummary)
}
```

Start with Models of means

```{r freemeans_Model1}
# residual variances
dataRaw      <- mxData( observed=alltask, type="raw" )
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; free to vary by test and occasion
                        labels=c("e1","e2","e3","e4","e5","e6","e7","e8","e9","e10","e11","e12") )
# each has a different name, meaning they are estimated with different values
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =mylabels ) 
myModel1 <- mxModel("Free means Model", type="RAM",
                    manifestVars=mylabels,
                    dataRaw, resVars,  means)
Model1Fit <- mxRun(myModel1)
summary.Model1<-summary(Model1Fit, refModels=mxRefModels(Model1Fit, run = TRUE)) #gives CFI, TLI, RMSEA if refModels specified - we won't use these as they aren't meaningful here
summary.Model1
#Define labels to add to bigtable
summary.Model1$Chi<- NA
summary.Model1$ChiDoF<-  NA
summary.Model1$p<- 1
summary.Model1$RMSEA<- NA
summary.Model1$BIC.Mx<- NA
summary.Model1$CFI<- NA
mycomp<-'-'
thiscomment<-'Free means/vars'
thisrowname<-'1.Independent data'
bigsummary<-addbig(summary.Model1,mycomp,thiscomment,thisrowname) 
#Note that the Model has 24 parameters (i.e. 12 means and 12 vars), but the Saturated Model has 90, as it
#also includes all the covariances between 12 variables.

```

N.B. The summary output shows estimates of variances (e) and means (M). Note that the variance is particularly high for SentGen2. Also v low variance for ListGen1

## Model 2: Tweak free means Model to check stability of means
 Means and variances set to be the same for time1 and time2 for each measure. 
 N.B. for this model and all subsequent means models, OpenMx gives a warning about possible misspecification of the model. This is because covariances are omitted from the model, but are included in the reference model. I think this need not be a problem, as we are anticipating v poor model fit, precisely because we ignore covariances, when the correlation matrix makes it clear that we need to model them. The models here, though, are intended just to compare between different theories about means, and to compare models that all suffer from this same deficiency. 
 
 
 This is achieved by giving the path the same name, e.g. meanA for A1 and A2
 N.B. This is not same as test-retest reliability: as covariances not considered.
 We're just testing if the mean values are similar time 1 and time 2, not whether same people are high or low.
 

```{r Model2}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("meanA","meanB","meanC",
                                  "meanD","meanE","meanF",
                                  "meanA","meanB","meanC",
                                  "meanD","meanE","meanF") ) 
myModel2 <- mxModel("Stability Model", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)
Model2Fit <- mxRun(myModel2)
summary.Model2<-summary(Model2Fit, refModels=mxRefModels(Model2Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2
test.1.2<-mxCompare(Model1Fit,Model2Fit)
summary.Model2$Chi<-test.1.2$diffLL[2]
summary.Model2$ChiDoF<-test.1.2$diffdf[2]
summary.Model2$p<-test.1.2$p[2]
mycomp<-'Model 1'
thiscomment<-'Equal means/vars'
thisrowname<-'2. Stable task effect'
bigsummary<-addbig(summary.Model2,mycomp,thiscomment,thisrowname) 

# Make a message (pmessage) that tells us what the Model comparison tells us. 
# Here we are looking for a nonsignificant p-value: that tells us that despite having fewer estimated parameters, fit does not suffer
# df is N observations (nsub*nvalues = 28*12) minus N estimated parameters (ep)
pmessage<-'Model 2 fit deteriorates relative to Model 1! Means differ across test occasions' #default message
if(test.1.2$p[2]>.05){
  pmessage <- 'Model 2 fit does not deteriorate relative to Model 1; ie means/vars equivalent across test occasions'}

pmessage

```
## Model 3: Model where all tests equivalent

More extreme version of stability Model, where all means and all vars are the same.
This tests (rather implausible!) hypothesis that all measures are similarly lateralised. It is equivalent to the 'population bias' Model.
We expect fit to worsen relative to stability Model - assuming measures differ in extent of laterality.
```{r Model3}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e1","e1","e1","e1","e1","e1","e1","e1","e1","e1","e1") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("meanA","meanA","meanA",
                                  "meanA","meanA","meanA","meanA","meanA","meanA",
                                  "meanA","meanA","meanA") ) 
myModel3 <- mxModel("Model3", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model3Fit <- mxRun(myModel3) #The mxRun command evaluates the Model.
summary.Model3<-summary(Model3Fit, refModels=mxRefModels(Model3Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2
#NB Model with most DF is last; Model 3 has many DF because no task-specific terms

test.2.3<-mxCompare(Model2Fit,Model3Fit)
summary.Model3$Chi<-test.2.3$diffLL[2]
summary.Model3$ChiDoF<-test.2.3$diffdf[2]
summary.Model3$p<-test.2.3$p[2]
mycomp<-'Model 2'
thiscomment<-'All means/vars equal'
thisrowname<-'3. Population bias'
bigsummary<-addbig(summary.Model3,mycomp,thiscomment,thisrowname) 

#We predict that means differ, in which case p will be < .05
pmessage<-'Means do not differ between tasks' #default
if(test.2.3$p[2]<.05){pmessage <- 'Means differ between tasks'}

pmessage
bigsummary
```
##Model 4: Test dorsal/ventral/mixed Model

 
This proposes pattern of means different for taskAB, C and DEF (Model 2a in prereg document).
Model predicts strength of laterality will be: AB > DEF > C.
We test this by equating means and variances for AB and DEF.

```{r dorsalventral}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e1","e2","e3","e3","e3","e1","e1","e2","e3","e3","e3") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("meanAB","meanAB","meanC",
                                  "meanDEF","meanDEF","meanDEF",
                                  "meanAB","meanAB","meanC",
                                  "meanDEF","meanDEF","meanDEF") ) 
myModel4 <- mxModel("Model4", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model4Fit <- mxRun(myModel4) #The mxRun command evaluates the Model.
summary.Model4<-summary(Model4Fit, refModels=mxRefModels(Model4Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.2.4<-mxCompare(Model2Fit,Model4Fit)
summary.Model4$Chi<-test.2.4$diffLL[2]
summary.Model4$ChiDoF<-test.2.4$diffdf[2]
summary.Model4$p<-test.2.4$p[2]
mycomp<-'Model 2'
thiscomment<-'AB>DEF>C'
thisrowname<-'4.Dorsal/ventral stream'
bigsummary<-addbig(summary.Model4,mycomp,thiscomment,thisrowname) 

#Add some output showing the mean estimates and testing if they fit the expected pattern
my4<-summary(Model4Fit)$parameters[c(4,6,5),c(1,5,6)]
qmessage<-'Model predicts AB > DEF > C'
rmessage<-'Not confirmed'
if((my4[1,2]>my4[3,2])&&(my4[3,2]>my4[2,2])){rmessage<-'Confirmed'}

pmessage
qmessage
rmessage
my4

```
##Model 5: Lexical retrieval Model
Hypothesis A.2 Strength of lateralization depends on the extent to which tasks require lexical retrieval (more lexical retrieval = stronger left lateralization).


Operationalised by setting means equal for tasks BD, ACF and E (Model 5)

```{r lexical retrieval}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=TRUE, values=rep(1,12), #estimate error variances; same for each test on time 1 and time 2
                        labels=c("e1","e2","e1","e2","e3","e2","e1","e2","e1","e2","e3","e2") )
# means
means        <- mxPath( from="one", to=mylabels, arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("meanACF","meanBD","meanACF",
                                  "meanBD","meanE","meanACF",
                                  "meanACF","meanBD","meanACF",
                                  "meanBD","meanE","meanACF") ) 
myModel5 <- mxModel("Model5", type="RAM",
                     manifestVars=mylabels,
                     dataRaw, resVars,  means)

Model5Fit <- mxRun(myModel5) #The mxRun command evaluates the Model.
summary.Model5<-summary(Model5Fit, refModels=mxRefModels(Model5Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.2.5<-mxCompare(Model2Fit,Model5Fit)
summary.Model5$Chi<-test.2.5$diffLL[2]
summary.Model5$ChiDoF<-test.2.5$diffdf[2]
summary.Model5$p<-test.2.5$p[2]
mycomp<-'Model 2'
thiscomment<-'BD>ACF'
thisrowname<-'5.LexicalRetrieval'
bigsummary<-addbig(summary.Model5,mycomp,thiscomment,thisrowname) 


#Add some output showing the mean estimates and testing if they fit the expected pattern
my5<-summary(Model5Fit)$parameters[c(5,4,6),c(1,5,6)] #pull out just the rows/cols of interest
qmessage<-'Model predicts BD > ACF'
rmessage<-'Not confirmed'
if(my5[1,2]>my5[2,2]){rmessage<-'Confirmed'}

pmessage
qmessage
rmessage
my5

```



#SEM Models including covariances
##Model 6: Single factor Model (means equalized for T1 and T2).

If tasks are correlated (as they are) this will give a better fit than the preceding Models, which only looks at means.
It is necessary to scale the paths from observed variables to latent factor in terms of one of the observed variables. We chose the first variable. NB I have checked that choice does not affect the factor solution.

```{r Factor1}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )

# latent variance - Factor1 is the single factor
latVar       <- mxPath( from="Factor1", arrows=2,
                        free=TRUE, values=1, labels ="varFactor1" )
# factor loadings
facLoads     <- mxPath( from="Factor1", to=mylabels, arrows=1,
                        free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                        values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels =c("l1","l2","l3","l4","l5","l6","l1","l2","l3","l4","l5","l6") )#same for each test on time 1 and 2
#The first path is fixed at one - others scaled relative to this

# means - one extra mean for the Factor, but this is set to NA
means        <- mxPath( from="one", to=c(mylabels,'Factor1'), arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T,FALSE), values=c(1,1,1,1,1,1,1,1,1,1,1,1,0),
                        labels =c("meanA","meanB","meanC",
                                  "meanD","meanE","meanF","meanA","meanB","meanC",
                                  "meanD","meanE","meanF",NA) ) #means constant from time 1 to time 2

myModel6 <- mxModel("Single Factor Model", type="RAM",
                          manifestVars=mylabels, latentVars="Factor1",
                          dataRaw, resVars, latVar, facLoads, means)
Model6Fit <- mxRun(myModel6)
summary.Model6<-summary(Model6Fit, refModels=mxRefModels(Model6Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.6.2<-mxCompare(Model6Fit,Model2Fit)
summary.Model6$Chi<-test.6.2$diffLL[2]
summary.Model6$ChiDoF<-test.6.2$diffdf[2]
summary.Model6$p<-test.6.2$p[2]
mycomp<-'Model 2'
thiscomment<-'Covariances: One factor'
thisrowname<-'6.Person Effect'
bigsummary<-addbig(summary.Model6,mycomp,thiscomment,thisrowname) 


pmessage<-'One factor Model no better than Model with no covariance between measures'
if(test.6.2$p[2]<.05){
  pmessage <- paste0('One factor Model is better fit than Model with no covariance between measures')
}
pmessage
bigsummary

```
Note how the CFI is now in a more sensible range, but still is well below acceptable level.
Will a bifactor Model improve this?

##Model 7: Bifactor Model
Here we specify two independent latent factors. Aim is to see whether the tests form two clusters.

```{r bifactor}
# residual variances
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e1","e2","e3","e4","e5","e6") )

# latent variances and covariance: NB assume totally independent, so covariance fixed at zero
latVars      <- mxPath( from=c("Factor1","Factor2"), arrows=2, connect="unique.pairs",
                        free=c(T,F,F), values=c(1,0,1), labels=c("varFactor1","cov","varFactor2") )
#changed the free statement from free =c(T,T,T) (that was error in prereg script: gives underidentified Model)

# factor loadings for Factor1 #NB test A loading is fixed to one for this factor
facLoadsFactor1     <- mxPath( from="Factor1", to=mylabels, arrows=1,
                               free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                               values=rep(1,12),
                               labels =c("k1","k2","k3","k4","k5","k6","k1","k2","k3","k4","k5","k6") )

# factor loadings for Factor2 #NB test A loading is fixed to zero for this factor
facLoadsFactor2     <- mxPath( from="Factor2", to=mylabels, arrows=1,
                               free=c(FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE), 
                               values=c(0,rep(1,5),0,rep(1,5)),
                               labels =c("l1","l2","l3","l4","l5","l6","l1","l2","l3","l4","l5","l6") )

# means #estimated for all except the two factors
means        <- mxPath( from="one", to=c(mylabels,'Factor1','Factor2'), arrows=1,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T,FALSE,FALSE), values=c(1,1,1,1,1,1,1,1,1,1,1,1,0,0),
                        labels =c("meanA","meanB","meanC",
                                  "meanD","meanE","meanF","meanA","meanB","meanC",
                                  "meanD","meanE","meanF",NA,NA) )

myModel7 <- mxModel("BiFactor Model", type="RAM",
                          manifestVars=mylabels,
                          latentVars=c("Factor1","Factor2"),
                          dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)

Model7Fit <- mxRun(myModel7)
summary.Model7<-summary(Model7Fit, refModels=mxRefModels(Model7Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.7.6<-mxCompare(Model7Fit,Model6Fit)
summary.Model7$Chi<-test.7.6$diffLL[2]
summary.Model7$ChiDoF<-test.7.6$diffdf[2]
summary.Model7$p<-test.7.6$p[2]
mycomp<-'Model 6'
thiscomment<-'Covariances: bifactor'
thisrowname<-'7.Task x Person Effect'
bigsummary<-addbig(summary.Model7,mycomp,thiscomment,thisrowname) 

pmessage<-'Bi-factor Model does not improve fit over one factor Model'

if (test.7.6$p[2]<.05){pmessage <- paste0('Bi-factor Model is better fit than one factor Model' )}
pmessage
test.7.6
bigsummary
#mxCheckIdentification(myModel7, details=TRUE) # check Model identification: all OK

```
Better fit for bifactor than for single factor, though CFI is still below accepted value for good fit. 
To explore reasons for poor fit, can look at differences between expected and observed covariances.

## Explore reasons for poor fit
We can extract the observed covariance matrix and compare it with the expected values from the current Model. If we take the difference between these, we can identify whether there are particular covariances that are problematic.
```{r covexplorefunction}
covexplore<-function(ModelFit,mytitle){

mymat.e<-mxGetExpected(ModelFit,"covariance")
mymat.o <- cov(alltask,use="pairwise.complete.obs")
mymat.d<-abs(mymat.e-mymat.o) #absolute size of mismatch between obs and expected
mymat.d1<-mymat.e-mymat.o #size of mismatch with sign
cc<-rainbow(ncol(mymat.d1))
h<-heatmap(mymat.d1,keep.dendro=FALSE,Rowv=NA,Colv=NA,
        revC=TRUE,col = cm.colors(256),margins=c(5,5),
        main=mytitle)
return(h)
}
```

```{r makeheatmap}
covexplore(Model7Fit,'Model 7: obs/exp covariance diffs')
```

Dark colours (pink or blue) indicate lack of agreement between obs and expected (neg or positive). Note the heat map show dark pink on the diagonal for SentGen1 and ListGen1. This means that the observed variance for these measures is lower than the Model predicts. The corresponding diagonal blocks for SentGen2 and PhonDec1 are blue: this is because we have set the estimated variances for the time 1 and time 2 tests to be the same (so based on average); so if observed value at one time is higher than the estimate, the estimate for the other time will be lower. When we look at the observed values in the stargazer table above, we can see that there are relatively large differences in variances for time1 and time2 for these two tasks. We can tweak one line of the Model, to allow variances to differ across occasions, and re-run. The fit does then improve. 
(We achieve this by just giving a different label to the path - see below)

Here is the Model with those settings:

##Model 8: Bifactor Model with unconstrained variances for T1 and T2


```{r bifactor2}

  #keep all the same except for resVars
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e11","e12","e13","e14","e15","e16") )

myModel8 <- mxModel("BiFactor Modelb", type="RAM",
                          manifestVars=mylabels,
                          latentVars=c("Factor1","Factor2"),
                          dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)

Model8Fit <- mxRun(myModel8)
summary.Model8<-summary(Model8Fit, refModels=mxRefModels(Model8Fit, run = TRUE)) 

#to get correct values for chi sq etc for model comparison run comparison and plug in values from row 2 of the summary

test.8.7<-mxCompare(Model8Fit,Model7Fit)
summary.Model8$Chi<-test.8.7$diffLL[2]
summary.Model8$ChiDoF<-test.8.7$diffdf[2]
summary.Model8$p<-test.8.7$p[2]
mycomp<-'Model 7'
thiscomment<-'Bifactor, vars differ by session'
thisrowname<-'8.Task x Person Effect, free vars'
bigsummary<-addbig(summary.Model8,mycomp,thiscomment,thisrowname) 

pmessage<-'Bi-factor Modelb does not improve fit over one factor Model'

pmessage <- paste0('Bi-factor Modelb  is better fit than one factor Model ')

if (test.8.7$p[2]<.05){psummary <-'Bi-factor Modelb gives better fit than one factor Model'}
pmessage
test.8.7
bigsummary
sumfilename<-"bigsummary.txt"
if(usemean==0){
  sumfilename<-"bigsummary_peak.txt"
}
write.table(bigsummary, sumfilename, sep="\t",quote=F) 
```


Next chunk just redoes the heatmap for the new Model. Note that the colours are scaled so we still see some strong colours, regardless of absolute values of differences. The darker colours don't form any coherent pattern, so there is no justification for further tweaks of the model.

```{r covexplore2}

covexplore(Model8Fit,'Model 8: obs/exp covariance diffs')

```

## Draw simplified path diagram
Draw diagram of the bi-factor Model 8, with nonsignificant paths omitted.
The criterion for a non-significant path is currently set to z = 1.65, but it is rather arbitrary, and the picture will look different depending on how stringent you make it.
N.B. The file for_graphviz is set up in advance and read in and modified according to results. For this part of the script to work you must have the file 'for_graphviz.csv' in your working directory.

Also, if this file is knitted as a word document, the figure won't render. To see it, knit document as html.

Test A is shown in red as this has fixed paths to X1 (1) and X2 (0).

NB If this figure is published, need to make following points:

* This is simplified path diagram. It shows just one measure per variable, when in fact there were two, and it does not show means, though these were estimated.

* Also nonsignificant paths are omitted.
```{r drawpaths}
require(stringr)

mysummary<-summary.Model8$parameters[1:10,c(1,3:6)]
if (usemean==0){
  mysummary<-summary.Model7$parameters[1:10,c(1,3:6)] #model 7 is best fit for peak values
}
mysummary$z<-mysummary$Estimate/mysummary$Std.Error
# omxGraphviz(biFactorModel, dotFilename = "bifactor.dot")
# grViz("bifactor.dot") #this will generate a .dot file but it
# is messy, as it shows time 1 and time 2 measures, as well as means

# Script below shows time1/time2 combined and omits means for clarity
mybit<-read.csv('for_graphviz.csv',stringsAsFactors = FALSE,header=FALSE) #full list of all paths.
mybit2<-print.data.frame(mybit, 
                 quote=FALSE) #get rid of quotes

#we now want to a) remove rows that are NS and b) put in path coeffs for the rest
thisrow<-12 #NB: first row with path specification is col 13
thatrow<-0 #counter for the summary z scores: NB these exclude measure A! 
for (j in 1:2){#each *factor* (not each test occasion - these are collapsed in diagram)

for (i in 1:6){ #each task 
    thisrow<-thisrow+1

    if(i>1){ #measure A is fixed so not in the table
      thatrow<-thatrow+1
    if(mysummary$z[thatrow]<1.65)
    {mybit2[thisrow,]<-''} #delete this one
    else{
      pathlabel<-round(mysummary$Estimate[thatrow],2)
      bb<-mybit2[thisrow,]
      bb<-str_replace(bb,'xx',as.character(pathlabel))
      mybit2[thisrow,]<-bb
    }
    } #loop to here when i is 1: no action
  }
}
mybit2[19,]<-'' #delete path for A to X2: this one was fixed to 0
dotFilename<-'dottry.dot'
if(usemean==0){
  dotFilename<-'dottry_peak.dot'
}
write.table(mybit2, dotFilename, append = FALSE,
            row.names = FALSE, col.names = FALSE,quote=FALSE)

grViz(dotFilename)

```

##Check solution robustness with drop one approach

Final bifactor model re-run with one subject dropped each time.
Standardized factor loading for each run saved in dropone_z.csv.
This reveals that on one run, when row 23 excluded, the model changes completely.
This is problematic: suggests bifactor pattern of results depends on inclusion of data from one specific participant.


```{r dropone, echo=FALSE}
do.dropone<-1
if (do.dropone==1){
#First create dataframe to save the results
droponesummary <- mysummary[,c(1:3,6)]
droponesummary$z<-round(droponesummary$z,2)
colnames(droponesummary)[4]<-'z.all'
droponesummary$row<-c('B','C','D','E','F','B','C','D','E','F')
#Use'drop one' approach - reduce the dataset by one on each run
#
PT_summary<-matrix(NA,ncol=29, nrow=30) #initialise matrix to gather estimates for each of 29 parameters for each of 30 runs

 for (thisdrop in 1:nrow(alltask)){
   dataRaw      <- mxData( observed=alltask[-thisdrop,], type="raw" )
 
    #keep all other parameters same as last model
resVars      <- mxPath( from=mylabels, arrows=2,
                        free=c(T,T,T,T,T,T,T,T,T,T,T,T), values=c(1,1,1,1,1,1,1,1,1,1,1,1),
                        labels=c("e1","e2","e3","e4","e5","e6","e11","e12","e13","e14","e15","e16") )

myModel8 <- mxModel("BiFactor Modelb", type="RAM",
                          manifestVars=mylabels,
                          latentVars=c("Factor1","Factor2"),
                          dataRaw, resVars, latVars, facLoadsFactor1, facLoadsFactor2, means)

Model8Fit <- mxRun(myModel8)
thissummary<-summary(Model8Fit)

PT_summary[thisdrop,]<-thissummary$parameters$Estimate

temp<-thissummary$parameters[1:10,c(1,3:6)]
nucol<-ncol(droponesummary)+1
droponesummary[,nucol]<-round(temp$Estimate/temp$Std.Error,2)
colnames(droponesummary)[nucol]<-paste0('z.',thisdrop)
 }
write.table(droponesummary, 'dropone_z', sep="\t",quote=F) 
}
t(droponesummary) #display transposed
```

##Look at data for row 23
Further exploratory analyses were conducted to gain insight into why row 23 data had a large effect on the solution

```{r checkindividuals}

#Add handedness data
alltask$hand<-2
w<- which(particdat$handedness=='R')
alltask$hand[w]<-1

meanSentGen<-rowMeans(alltask[,c(4,10)],na.rm=F)
meanJabberwocky<-rowMeans(alltask[,c(6,12)],na.rm=T)
plot(meanSentGen,meanJabberwocky,col=alltask$hand,pch=15)
text(-2.5,2,'Row 23')
```

##Make factor scores and plot them
In a further exploratory analysis, we plotted factor scores by making a weighted sum of scores, using factor loadings as weights.
This shows that although the factors are modeled to be independent, they are highly intercorrelated. Handedness is colour coded. Inspection of this figure suggests that left- and right-handers should be modeled separately. Indeed, if this script is rerun with right-handers only, then single factor model gives best solution.

```{r makefacscores}
#? Use loadings or zscores?
#This is with loadings - but it makes little difference
#In weighted sum use original data, including excluded datapoints (alltaskall) to avoid losing subjects from this
allz1<-c(1,mysummary$Estimate[1:5])
allz2<-c(0,mysummary$Estimate[6:10]) #1st and 6th values for task A
for (i in 1:nrow(alltask)){
alltask$f1score[i]<-alltaskall[i,1]*allz1[1]+
  alltaskall[i,2]*allz1[2]+
  alltaskall[i,3]*allz1[3]+
  alltaskall[i,4]*allz1[4]+
  alltaskall[i,5]*allz1[5]+
  alltaskall[i,6]*allz1[6]
alltask$f2score[i]<-alltaskall[i,1]*allz2[1]+
  alltaskall[i,2]*allz2[2]+
  alltaskall[i,3]*allz2[3]+
  alltaskall[i,4]*allz2[4]+
  alltaskall[i,5]*allz2[5]+
  alltaskall[i,6]*allz2[6]
}


plot(alltask$f1score,alltask$f2score,col=alltask$hand,pch=15)
abline(v=0,lty=2)
abline(h=0,lty=2)
cor(alltask$f1score,alltask$f2score,use='pairwise.complete.obs')

```

##Jacknife estimates of path SEs
Jacknife estimates considered for model SEs, which can be compared with standard method. The jacknife method uses the output from the drop-one analysis to look at distribution of estimates obtained with one subject omitted.
It confirms that there are particular problems with estimates of paths from Jabberwocky to Factor 1 and from Sentence Generation to Factor 2.


```{R jackknife}

  # Calculates the Jackknife estimates (bias corrected) and the bias. Then prints out corrected estimates vs the "real" estimates from the original model 8 run.

 n=30
 real_est<-summary.Model8$parameters$Estimate #estimate with all cases included
 #PT_summary
 PT_means<-colMeans(PT_summary) #average estimate across all dropone runs
 JK_bias<-(n-1)*(PT_means-real_est) 
 
 Jack_est<-(n*real_est)-((n-1)*PT_means)

 plot(real_est,Jack_est)
 
 
 # calculates the Jackknife SE estimates to allow us to calculate the CI.
 
 #PT_summary
 Jack_SE<-vector(mode="numeric",length=29)
 for (i in 1:29)
 {
 Jack_SE[i]<-sqrt(((n-1)/n)*sum((PT_summary[,i]-PT_means[i])^2))
 }
 myres<-summary.Model8$parameters[,1:6]
 myres$L_ci<-round(summary.Model8$parameters$Estimate-1.65*summary.Model8$parameters$Std.Error,2)
 myres$U_ci<-round(summary.Model8$parameters$Estimate+1.65*summary.Model8$parameters$Std.Error,2)
 myres$Jack_Estimate<-round(Jack_est,2)
 myres$Jack_Std.Error<-round(Jack_SE,2)
 myres$JK_L_ci<-round(Jack_est-1.65*Jack_SE,2)
 myres$JK_U_ci<-round(Jack_est+1.65*Jack_SE,2)
 myres$JK_bias<-round(JK_bias,2)
 myres$Estimate<-round(myres$Estimate,2)
 
 myres[,c(1,3,4,5,9)]

 write.csv(myres,"Jacknife_ests.csv")
```

```{r compare_baselines}

olddir<-"~/Dropbox/Project A2/A2_Data/original_baseline/"
oldbase1<-read.csv(paste0(olddir,'Results_Session1_baseline_-5_0.csv'))
oldbase2<-read.csv(paste0(olddir,'Results_Session2_baseline_-5_0.csv'))
allse.old <- cbind(select(oldbase1,A1.se,B1.se,C1.se,D1.se,E1.se,F1.se),
             select(oldbase2,A2.se,B2.se,C2.se,D2.se,E2.se,F2.se))
if (usemean==1){
allse <- cbind(select(oldbase1,A1.mean_se,B1.mean_se,C1.mean_se,D1.mean_se,E1.mean_se,F1.mean_se),
             select(oldbase2,A2.mean_se,B2.mean_se,C2.mean_se,D2.mean_se,E2.mean_se,F2.mean_se))
}
myse.old<-c(allse[,1],allse[,2],allse[,3],allse[,4],allse[,5],allse[,6],
        allse[,7],allse[,8],allse[,9],allse[,10],allse[,11],allse[,12])


newold<-data.frame(cbind(myse,myse.old))
colnames(newold)<-c('new baseline se','old baseline se')

 png(filename="baseline_se_compare.png")
plot(newold[,2],newold[,1],xlab='Old baseline (-5 to 0 s)',ylab='New baseline (-5 to 2 s)')
abline(coef=c(0,1))
dev.off()

```

##Session information
```{r sessinfo}
sessionInfo()
```